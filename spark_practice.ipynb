{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries Required\n",
    "import os\n",
    "\n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "\n",
    "pd.set_option(\"display.max_rows\", 2000)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns   \n",
    "import pickle \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://INGGNLT1PYD593.INFO.CORP:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ff7feab280>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is OSDisk\n",
      " Volume Serial Number is D86A-C798\n",
      "\n",
      " Directory of c:\\Users\\akash.mathur\\Downloads\\Akash\\Spark\n",
      "\n",
      "08/14/2022  05:27 PM    <DIR>          .\n",
      "08/14/2022  05:27 PM    <DIR>          ..\n",
      "08/10/2022  07:56 PM         3,019,965 canada_forecast_NEW.parquet\n",
      "08/12/2022  05:16 PM        65,699,395 LoanStats_2018Q4.csv\n",
      "08/14/2022  05:27 PM         1,853,425 spark.docx\n",
      "08/14/2022  05:26 PM            82,036 spark_practice.ipynb\n",
      "               4 File(s)     70,654,821 bytes\n",
      "               2 Dir(s)  24,078,413,824 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_location = \"LoanStats_2018Q4.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# df = spark.read.format('csv')\\\n",
    "#     .option(\"inferSchema\", infer_schema)\\\n",
    "#     .option(\"header\", first_row_is_header)\\\n",
    "#     .option(\"sep\", delimiter)\\\n",
    "#     .load(file_location)\n",
    "    \n",
    "df = spark.read.format('csv')\\\n",
    "    .options(inferSchema= infer_schema,\n",
    "    header= first_row_is_header,\n",
    "    sep= delimiter)\\\n",
    "    .load(file_location)\n",
    "\n",
    "# ALTERNATE\n",
    "# spark.read.load(\"LoanStats_2018Q4.csv\",\n",
    "#                      format=\"csv\", sep=\";\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "# spark.read.csv(\"LoanStats_2018Q4.csv\",\n",
    "#                      inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[loan_amnt: int, funded_amnt: int, funded_amnt_inv: int, term: string, int_rate: string, installment: double, grade: string, sub_grade: string, emp_title: string, emp_length: string, home_ownership: string, annual_inc: int, verification_status: string, issue_d: string, loan_status: string, pymnt_plan: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loan_amnt: integer (nullable = true)\n",
      " |-- funded_amnt: integer (nullable = true)\n",
      " |-- funded_amnt_inv: integer (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- int_rate: string (nullable = true)\n",
      " |-- installment: double (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- sub_grade: string (nullable = true)\n",
      " |-- emp_title: string (nullable = true)\n",
      " |-- emp_length: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: integer (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- issue_d: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- pymnt_plan: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cached in memory (Persists the DataFrame with the default storage level (MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.format('csv')\\\n",
    "    .option(\"inferSchema\", infer_schema)\\\n",
    "    .option(\"header\", first_row_is_header)\\\n",
    "    .option(\"sep\", delimiter)\\\n",
    "    .load(file_location).cache()\n",
    "\n",
    "#OR\n",
    "    \n",
    "df1.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling for Conversion to/from Pandas\n",
    "\n",
    "Not recommended, because you will lose out on memory. If you need to just sample a few rows and convert to pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Generate a Pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a Pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Convert the Spark DataFrame back to a Pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas() #OR\n",
    "df_pa = df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.alias('df1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[loan_amnt: int, funded_amnt: int, funded_amnt_inv: int, term: string, int_rate: string, installment: double, grade: string, sub_grade: string, emp_title: string, emp_length: string, home_ownership: string, annual_inc: int, verification_status: string, issue_d: string, loan_status: string, pymnt_plan: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# only applicable for Databricks\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[loan_amnt: int, funded_amnt: int, funded_amnt_inv: int, term: string, int_rate: string, installment: double, grade: string, sub_grade: string, emp_title: string, emp_length: string, home_ownership: string, annual_inc: int, verification_status: string, issue_d: string, loan_status: string, pymnt_plan: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(loan_amnt=10000, funded_amnt=10000, funded_amnt_inv=10000, term=' 36 months', int_rate='10.33%', installment=324.23, grade='B', sub_grade='B1', emp_title=None, emp_length='< 1 year', home_ownership='MORTGAGE', annual_inc=280000, verification_status='Not Verified', issue_d='18-Dec', loan_status='Current', pymnt_plan='n')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check DataTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loan_amnt', 'int'),\n",
       " ('funded_amnt', 'int'),\n",
       " ('funded_amnt_inv', 'int'),\n",
       " ('term', 'string'),\n",
       " ('int_rate', 'string'),\n",
       " ('installment', 'double'),\n",
       " ('grade', 'string'),\n",
       " ('sub_grade', 'string'),\n",
       " ('emp_title', 'string'),\n",
       " ('emp_length', 'string'),\n",
       " ('home_ownership', 'string'),\n",
       " ('annual_inc', 'int'),\n",
       " ('verification_status', 'string'),\n",
       " ('issue_d', 'string'),\n",
       " ('loan_status', 'string'),\n",
       " ('pymnt_plan', 'string')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loan_amnt: integer (nullable = true)\n",
      " |-- funded_amnt: integer (nullable = true)\n",
      " |-- funded_amnt_inv: integer (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- int_rate: string (nullable = true)\n",
      " |-- installment: double (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- sub_grade: string (nullable = true)\n",
      " |-- emp_title: string (nullable = true)\n",
      " |-- emp_length: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: integer (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- issue_d: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- pymnt_plan: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(emp_title=None, grade='B', sub_grade='B1'),\n",
       " Row(emp_title='Chef', grade='C', sub_grade='C1')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Collect (Returns all the records as a list of Row)\n",
    "\n",
    "df.select(['emp_title', 'grade', 'sub_grade']).limit(2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---------+\n",
      "|emp_title|grade|sub_grade|\n",
      "+---------+-----+---------+\n",
      "|     null|    B|       B1|\n",
      "|     Chef|    C|       C1|\n",
      "+---------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['emp_title', 'grade', 'sub_grade']).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---------+\n",
      "|emp_title|grade|sub_grade|\n",
      "+---------+-----+---------+\n",
      "|     null|    B|       B1|\n",
      "|     Chef|    C|       C1|\n",
      "+---------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([col('emp_title'), col('grade'), col('sub_grade')]).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['emp_title']#.show(2)\n",
    "type(df['emp_title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Top N Rows in Spark/PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|loan_amnt|funded_amnt|funded_amnt_inv|      term|int_rate|installment|grade|sub_grade|emp_title|emp_length|home_ownership|annual_inc|verification_status|issue_d|loan_status|pymnt_plan|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|    10000|      10000|          10000| 36 months|  10.33%|     324.23|    B|       B1|     null|  < 1 year|      MORTGAGE|    280000|       Not Verified| 18-Dec|    Current|         n|\n",
      "|     2500|       2500|           2500| 36 months|  13.56%|      84.92|    C|       C1|     Chef| 10+ years|          RENT|     55000|       Not Verified| 18-Dec|    Current|         n|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show() -> Print/Show top 20 rows in a tabular form\n",
    "# show(n) -> Print/Show top N rows in a tabular form\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take/first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take() Returns top N row.\tPySpark – Return list of Row **ACTION**\n",
    "# first() Returns the first row\tPySpark – Return list of Row **ACTION**\n",
    "\n",
    "# head()\tReturns the first row\tPySpark – Return list of Row **ACTION**\n",
    "# head(n)\tReturns top N rows PySpark – Return list of Row **ACTION**\n",
    "# tail(n)\tReturns Last N rows\tPySpark – Return list of class Row **ACTION**\n",
    "\n",
    "# collect()\tReturns all dataset\tPySpark – Return All as a list of Row **ACTION**\n",
    "\n",
    "# limit(n)\tReturns Top N rows\tPySpark – Returns a new DataFrame **Transformation**\n",
    "\n",
    "# Note: take(), first(), tail() and head() actions internally calls limit() transformation and finally calls collect() action to collect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(loan_amnt=10000, funded_amnt=10000, funded_amnt_inv=10000, term=' 36 months', int_rate='10.33%', installment=324.23, grade='B', sub_grade='B1', emp_title=None, emp_length='< 1 year', home_ownership='MORTGAGE', annual_inc=280000, verification_status='Not Verified', issue_d='18-Dec', loan_status='Current', pymnt_plan='n')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(loan_amnt=10000, funded_amnt=10000, funded_amnt_inv=10000, term=' 36 months', int_rate='10.33%', installment=324.23, grade='B', sub_grade='B1', emp_title=None, emp_length='< 1 year', home_ownership='MORTGAGE', annual_inc=280000, verification_status='Not Verified', issue_d='18-Dec', loan_status='Current', pymnt_plan='n')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(loan_amnt=10000, funded_amnt=10000, funded_amnt_inv=10000, term=' 36 months', int_rate='10.33%', installment=324.23, grade='B', sub_grade='B1', emp_title=None, emp_length='< 1 year', home_ownership='MORTGAGE', annual_inc=280000, verification_status='Not Verified', issue_d='18-Dec', loan_status='Current', pymnt_plan='n')]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(loan_amnt=22000, funded_amnt=22000, funded_amnt_inv=22000, term=' 60 months', int_rate='11.80%', installment=487.16, grade='B', sub_grade='B4', emp_title='Corporate Communications Strategist', emp_length='10+ years', home_ownership='MORTGAGE', annual_inc=155000, verification_status='Source Verified', issue_d='18-Dec', loan_status='Current', pymnt_plan='n')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[loan_amnt: int, funded_amnt: int, funded_amnt_inv: int, term: string, int_rate: string, installment: double, grade: string, sub_grade: string, emp_title: string, emp_length: string, home_ownership: string, annual_inc: int, verification_status: string, issue_d: string, loan_status: string, pymnt_plan: string]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(loan_amnt=10000, funded_amnt=10000, funded_amnt_inv=10000, term=' 36 months', int_rate='10.33%', installment=324.23, grade='B', sub_grade='B1', emp_title=None, emp_length='< 1 year', home_ownership='MORTGAGE', annual_inc=280000, verification_status='Not Verified', issue_d='18-Dec', loan_status='Current', pymnt_plan='n')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(loan_amnt=10000, funded_amnt=10000, funded_amnt_inv=10000, term=' 36 months', int_rate='10.33%', installment=324.23, grade='B', sub_grade='B1', emp_title=None, emp_length='< 1 year', home_ownership='MORTGAGE', annual_inc=280000, verification_status='Not Verified', issue_d='18-Dec', loan_status='Current', pymnt_plan='n')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.limit(1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|loan_amnt|funded_amnt|funded_amnt_inv|      term|int_rate|installment|grade|sub_grade|emp_title|emp_length|home_ownership|annual_inc|verification_status|issue_d|loan_status|pymnt_plan|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|    10000|      10000|          10000| 36 months|  10.33%|     324.23|    B|       B1|     null|  < 1 year|      MORTGAGE|    280000|       Not Verified| 18-Dec|    Current|         n|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           emp_title|\n",
      "+--------------------+\n",
      "|                null|\n",
      "|                Chef|\n",
      "|                null|\n",
      "|                null|\n",
      "|Instructional Coo...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('emp_title').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|emp_title|\n",
      "+---------+\n",
      "|     null|\n",
      "|     Chef|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('emp_title').limit(2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(emp_title=None)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('emp_title').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.select('emp_title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(loan_amnt=10000, funded_amnt=10000, funded_amnt_inv=10000, term=' 36 months', int_rate='10.33%', installment=324.23, grade='B', sub_grade='B1', emp_title=None, emp_length='< 1 year', home_ownership='MORTGAGE', annual_inc=280000, verification_status='Not Verified', issue_d='18-Dec', loan_status='Current', pymnt_plan='n'),\n",
       " Row(loan_amnt=2500, funded_amnt=2500, funded_amnt_inv=2500, term=' 36 months', int_rate='13.56%', installment=84.92, grade='C', sub_grade='C1', emp_title='Chef', emp_length='10+ years', home_ownership='RENT', annual_inc=55000, verification_status='Not Verified', issue_d='18-Dec', loan_status='Current', pymnt_plan='n')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns the list of Row objects\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           emp_title|\n",
      "+--------------------+\n",
      "|        UPS/Shipping|\n",
      "|Corporate Communi...|\n",
      "|  Nursing Supervisor|\n",
      "| Sale Representative|\n",
      "|      Administrative|\n",
      "|         Casino Host|\n",
      "|Operations Coordi...|\n",
      "|Help Tech Supervisor|\n",
      "|Program Support A...|\n",
      "|            Security|\n",
      "|            OPERATOR|\n",
      "|            Operator|\n",
      "|           Carpenter|\n",
      "|              Driver|\n",
      "|             Teacher|\n",
      "|Instructional Coo...|\n",
      "|                Chef|\n",
      "|                null|\n",
      "|              BANKER|\n",
      "|       Asst. Manager|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('emp_title').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|emp_title                          |\n",
      "+-----------------------------------+\n",
      "|UPS/Shipping                       |\n",
      "|Corporate Communications Strategist|\n",
      "|Nursing Supervisor                 |\n",
      "|Sale Representative                |\n",
      "|Administrative                     |\n",
      "|Casino Host                        |\n",
      "|Operations Coordinator             |\n",
      "|Help Tech Supervisor               |\n",
      "|Program Support Assistant          |\n",
      "|Security                           |\n",
      "|OPERATOR                           |\n",
      "|Operator                           |\n",
      "|Carpenter                          |\n",
      "|Driver                             |\n",
      "|Teacher                            |\n",
      "|Instructional Coordinator          |\n",
      "|Chef                               |\n",
      "|null                               |\n",
      "|BANKER                             |\n",
      "|Asst. Manager                      |\n",
      "+-----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('emp_title').distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[loan_amnt: int, funded_amnt: int, funded_amnt_inv: int, term: string, int_rate: string, installment: double, grade: string, sub_grade: string, emp_title: string, emp_length: string, home_ownership: string, annual_inc: int, verification_status: string, issue_d: string, loan_status: string, pymnt_plan: string]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('*')#.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------+\n",
      "|emp_title         |total_count|\n",
      "+------------------+-----------+\n",
      "|null              |20950      |\n",
      "|Teacher           |2090       |\n",
      "|Manager           |1773       |\n",
      "|Registered Nurse  |952        |\n",
      "|Driver            |924        |\n",
      "|RN                |726        |\n",
      "|Supervisor        |697        |\n",
      "|Sales             |580        |\n",
      "|Project Manager   |526        |\n",
      "|General Manager   |523        |\n",
      "|Office Manager    |521        |\n",
      "|Owner             |420        |\n",
      "|Director          |402        |\n",
      "|Operations Manager|387        |\n",
      "|Truck Driver      |387        |\n",
      "|Nurse             |326        |\n",
      "|Engineer          |325        |\n",
      "|Sales Manager     |304        |\n",
      "|manager           |301        |\n",
      "|Supervisor        |270        |\n",
      "+------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('emp_title').count().withColumnRenamed('count','total_count').orderBy('total_count', ascending=False).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a new DF with updated columns\n",
    "df.toDF('f1', 'f2', 'f3')\n",
    "\n",
    "# update individual columns\n",
    "df.withColumnRenamed('old','new')\n",
    "\n",
    "# using selectExpr\n",
    "df.selectExpr(\"old1 as new1\", \"old2 as new2\", \"old3 as new3\")\n",
    "\n",
    "# using selectExpr\n",
    "df.select(col(\"old1\").alias(\"new1\"), col(\"old2\").alias(\"new2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No axis column in pyspark\n",
    "df.drop('column')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, member_id: string, loan_amnt: int, funded_amnt: int, funded_amnt_inv: double, term: string, int_rate: string, installment: double, grade: string, sub_grade: string, emp_title: string, emp_length: string, home_ownership: string, annual_inc: double, verification_status: string, issue_d: string, loan_status: string, pymnt_plan: string, url: string, desc: string, purpose: string, title: string, zip_code: string, addr_state: string, dti: double, delinq_2yrs: int, earliest_cr_line: string, inq_last_6mths: int, mths_since_last_delinq: int, mths_since_last_record: int, open_acc: int, pub_rec: int, revol_bal: int, revol_util: string, total_acc: int, initial_list_status: string, out_prncp: double, out_prncp_inv: double, total_pymnt: double, total_pymnt_inv: double, total_rec_prncp: double, total_rec_int: double, total_rec_late_fee: double, recoveries: double, collection_recovery_fee: double, last_pymnt_d: string, last_pymnt_amnt: double, next_pymnt_d: string, last_credit_pull_d: string, collections_12_mths_ex_med: int, mths_since_last_major_derog: int, policy_code: int, application_type: string, annual_inc_joint: double, dti_joint: double, verification_status_joint: string, acc_now_delinq: int, tot_coll_amt: int, tot_cur_bal: int, open_acc_6m: int, open_act_il: int, open_il_12m: int, open_il_24m: int, mths_since_rcnt_il: int, total_bal_il: int, il_util: int, open_rv_12m: int, open_rv_24m: int, max_bal_bc: int, all_util: int, total_rev_hi_lim: int, inq_fi: int, total_cu_tl: int, inq_last_12m: int, acc_open_past_24mths: int, avg_cur_bal: int, bc_open_to_buy: int, bc_util: double, chargeoff_within_12_mths: int, delinq_amnt: int, mo_sin_old_il_acct: int, mo_sin_old_rev_tl_op: int, mo_sin_rcnt_rev_tl_op: int, mo_sin_rcnt_tl: int, mort_acc: int, mths_since_recent_bc: int, mths_since_recent_bc_dlq: int, mths_since_recent_inq: int, mths_since_recent_revol_delinq: int, num_accts_ever_120_pd: int, num_actv_bc_tl: int, num_actv_rev_tl: int, num_bc_sats: int, num_bc_tl: int, num_il_tl: int, num_op_rev_tl: int, num_rev_accts: int, num_rev_tl_bal_gt_0: int, num_sats: int, num_tl_120dpd_2m: int, num_tl_30dpd: int, num_tl_90g_dpd_24m: int, num_tl_op_past_12m: int, pct_tl_nvr_dlq: double, percent_bc_gt_75: double, pub_rec_bankruptcies: int, tax_liens: int, tot_hi_cred_lim: int, total_bal_ex_mort: int, total_bc_limit: int, total_il_high_credit_limit: int, revol_bal_joint: int, sec_app_earliest_cr_line: string, sec_app_inq_last_6mths: int, sec_app_mort_acc: int, sec_app_open_acc: int, sec_app_revol_util: double, sec_app_open_act_il: int, sec_app_num_rev_accts: int, sec_app_chargeoff_within_12_mths: int, sec_app_collections_12_mths_ex_med: int, sec_app_mths_since_last_major_derog: int, hardship_flag: string, hardship_type: string, hardship_reason: string, hardship_status: string, deferral_term: int, hardship_amount: double, hardship_start_date: string, hardship_end_date: string, payment_plan_start_date: string, hardship_length: int, hardship_dpd: int, hardship_loan_status: string, orig_projected_additional_accrued_interest: double, hardship_payoff_balance_amount: double, hardship_last_payment_amount: double, disbursement_method: string, debt_settlement_flag: string, debt_settlement_flag_date: string, settlement_status: string, settlement_date: string, settlement_amount: int, settlement_percentage: double, settlement_term: int]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df.settlement_percentage >0.5) & (df.settlement_amount==100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add columns\n",
    "\n",
    "In pandas - division by 0 gives infinity\n",
    "\n",
    "In spark - gives null, SQL like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+-------------+\n",
      "|loan_amnt|funded_amnt|funded_amnt_inv|      term|int_rate|installment|grade|sub_grade|emp_title|emp_length|home_ownership|annual_inc|verification_status|issue_d|loan_status|pymnt_plan|new_loan_amnt|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+-------------+\n",
      "|    10000|      10000|          10000| 36 months|  10.33%|     324.23|    B|       B1|     null|  < 1 year|      MORTGAGE|    280000|       Not Verified| 18-Dec|    Current|         n|       5000.0|\n",
      "|     2500|       2500|           2500| 36 months|  13.56%|      84.92|    C|       C1|     Chef| 10+ years|          RENT|     55000|       Not Verified| 18-Dec|    Current|         n|       1250.0|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# it will create a new DF, wont change the existing DF, because these are immutable\n",
    "df.withColumn('new_loan_amnt', (df.loan_amnt *0.5)).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Missing Values\n",
    "\n",
    "In PySpark DataFrame you can calculate the count of Null, None, NaN or Empty/Blank values in a column by using isNull() of Column class & SQL functions isnan() count() and when(). In this article, I will explain how to get the count of Null, None, NaN, empty or blank values from all or multiple selected columns of PySpark DataFrame.\n",
    "\n",
    "Note: In Python None is equal to null value, son on PySpark DataFrame None values are shown as null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+----+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|loan_amnt|funded_amnt|funded_amnt_inv|term|int_rate|installment|grade|sub_grade|emp_title|emp_length|home_ownership|annual_inc|verification_status|issue_d|loan_status|pymnt_plan|\n",
      "+---------+-----------+---------------+----+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|        0|          0|              0|   0|       0|          0|    0|        0|       20|         0|             0|         0|                  0|      0|          0|         0|\n",
      "+---------+-----------+---------------+----+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|loan_amnt|funded_amnt|funded_amnt_inv|      term|int_rate|installment|grade|sub_grade|emp_title|emp_length|home_ownership|annual_inc|verification_status|issue_d|loan_status|pymnt_plan|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|    10000|      10000|          10000| 36 months|  10.33%|     324.23|    B|       B1|     null|  < 1 year|      MORTGAGE|    280000|       Not Verified| 18-Dec|    Current|         n|\n",
      "|    12000|      12000|          12000| 60 months|  13.56%|     276.49|    C|       C1|     null|  < 1 year|      MORTGAGE|     40000|       Not Verified| 18-Dec|    Current|         n|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['emp_title'].isNull()).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+----+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|loan_amnt|funded_amnt|funded_amnt_inv|term|int_rate|installment|grade|sub_grade|emp_title|emp_length|home_ownership|annual_inc|verification_status|issue_d|loan_status|pymnt_plan|\n",
      "+---------+-----------+---------------+----+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "+---------+-----------+---------------+----+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(isnan('emp_title')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, member_id: string, loan_amnt: int, funded_amnt: int, funded_amnt_inv: double, term: string, int_rate: string, installment: double, grade: string, sub_grade: string, emp_title: string, emp_length: string, home_ownership: string, annual_inc: double, verification_status: string, issue_d: string, loan_status: string, pymnt_plan: string, url: string, desc: string, purpose: string, title: string, zip_code: string, addr_state: string, dti: double, delinq_2yrs: int, earliest_cr_line: string, inq_last_6mths: int, mths_since_last_delinq: int, mths_since_last_record: int, open_acc: int, pub_rec: int, revol_bal: int, revol_util: string, total_acc: int, initial_list_status: string, out_prncp: double, out_prncp_inv: double, total_pymnt: double, total_pymnt_inv: double, total_rec_prncp: double, total_rec_int: double, total_rec_late_fee: double, recoveries: double, collection_recovery_fee: double, last_pymnt_d: string, last_pymnt_amnt: double, next_pymnt_d: string, last_credit_pull_d: string, collections_12_mths_ex_med: int, mths_since_last_major_derog: int, policy_code: int, application_type: string, annual_inc_joint: double, dti_joint: double, verification_status_joint: string, acc_now_delinq: int, tot_coll_amt: int, tot_cur_bal: int, open_acc_6m: int, open_act_il: int, open_il_12m: int, open_il_24m: int, mths_since_rcnt_il: int, total_bal_il: int, il_util: int, open_rv_12m: int, open_rv_24m: int, max_bal_bc: int, all_util: int, total_rev_hi_lim: int, inq_fi: int, total_cu_tl: int, inq_last_12m: int, acc_open_past_24mths: int, avg_cur_bal: int, bc_open_to_buy: int, bc_util: double, chargeoff_within_12_mths: int, delinq_amnt: int, mo_sin_old_il_acct: int, mo_sin_old_rev_tl_op: int, mo_sin_rcnt_rev_tl_op: int, mo_sin_rcnt_tl: int, mort_acc: int, mths_since_recent_bc: int, mths_since_recent_bc_dlq: int, mths_since_recent_inq: int, mths_since_recent_revol_delinq: int, num_accts_ever_120_pd: int, num_actv_bc_tl: int, num_actv_rev_tl: int, num_bc_sats: int, num_bc_tl: int, num_il_tl: int, num_op_rev_tl: int, num_rev_accts: int, num_rev_tl_bal_gt_0: int, num_sats: int, num_tl_120dpd_2m: int, num_tl_30dpd: int, num_tl_90g_dpd_24m: int, num_tl_op_past_12m: int, pct_tl_nvr_dlq: double, percent_bc_gt_75: double, pub_rec_bankruptcies: int, tax_liens: int, tot_hi_cred_lim: int, total_bal_ex_mort: int, total_bc_limit: int, total_il_high_credit_limit: int, revol_bal_joint: int, sec_app_earliest_cr_line: string, sec_app_inq_last_6mths: int, sec_app_mort_acc: int, sec_app_open_acc: int, sec_app_revol_util: double, sec_app_open_act_il: int, sec_app_num_rev_accts: int, sec_app_chargeoff_within_12_mths: int, sec_app_collections_12_mths_ex_med: int, sec_app_mths_since_last_major_derog: int, hardship_flag: string, hardship_type: string, hardship_reason: string, hardship_status: string, deferral_term: int, hardship_amount: double, hardship_start_date: string, hardship_end_date: string, payment_plan_start_date: string, hardship_length: int, hardship_dpd: int, hardship_loan_status: string, orig_projected_additional_accrued_interest: double, hardship_payoff_balance_amount: double, hardship_last_payment_amount: double, disbursement_method: string, debt_settlement_flag: string, debt_settlement_flag_date: string, settlement_status: string, settlement_date: string, settlement_amount: int, settlement_percentage: double, settlement_term: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # how=any, by default, Will delete all rows where there is atleast 1 Null \n",
    "df.na.drop()\n",
    "\n",
    "# how=all, Will delete all rows where ALL cols are Null \n",
    "df.na.drop(how=\"all\")\n",
    "\n",
    "# atleast 2 Non-Null values should be present across cols, else it will delete the row\n",
    "df.na.drop(how=\"any\", thresh=2)\n",
    "\n",
    "# whereever there is Null values across subset col, it will delete\n",
    "df.na.drop(how=\"any\", subset=[\"member_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Nulls\n",
    "\n",
    "Additional options in pandas, wrt pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|     id|member_id|\n",
      "+-------+---------+\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "|Missing|  Missing|\n",
      "+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(\"Missing\", subset=['id','member_id']).select(['id','member_id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another dict way \n",
    "df.na.fill({\"id\":\"Missing\", \"member_id\":0}).select(['id','member_id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0034251235906457385"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr('settlement_amount','funded_amnt_inv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Transformations\n",
    "\n",
    "You can use numpy, but it will be slow. Use the built in functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loan_amnt',\n",
       " 'funded_amnt',\n",
       " 'funded_amnt_inv',\n",
       " 'term',\n",
       " 'int_rate',\n",
       " 'installment',\n",
       " 'grade',\n",
       " 'sub_grade',\n",
       " 'emp_title',\n",
       " 'emp_length',\n",
       " 'home_ownership',\n",
       " 'annual_inc',\n",
       " 'verification_status',\n",
       " 'issue_d',\n",
       " 'loan_status',\n",
       " 'pymnt_plan']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|grade|\n",
      "+-----+\n",
      "|    B|\n",
      "|    C|\n",
      "|    C|\n",
      "|    C|\n",
      "|    D|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"grade\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+-------------+\n",
      "|loan_amnt|funded_amnt|funded_amnt_inv|      term|int_rate|installment|grade|sub_grade|emp_title|emp_length|home_ownership|annual_inc|verification_status|issue_d|loan_status|pymnt_plan|new_loan_amnt|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+-------------+\n",
      "|    10000|      10000|          10000| 36 months|  10.33%|     324.23|    B|       B1|     null|  < 1 year|      MORTGAGE|    280000|       Not Verified| 18-Dec|    Current|         n|       1000.0|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#withColumn is a TRANSFORMATION\n",
    "\n",
    "df1 = df.withColumn('new_loan_amnt', (col('loan_amnt')*0.1))\n",
    "df1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[loan_amnt: int, funded_amnt: int, funded_amnt_inv: int, term: string, int_rate: string, installment: double, grade: string, sub_grade: string, emp_title: string, emp_length: string, home_ownership: string, annual_inc: int, verification_status: string, issue_d: string, loan_status: string, pymnt_plan: string, new_loan_amnt: double]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn('new_loan_amnt', (col('loan_amnt')*0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[loan_amnt: int, funded_amnt: int, funded_amnt_inv: int, term: string, int_rate: string, installment: double, grade: string, sub_grade: string, emp_title: string, emp_length: string, home_ownership: string, annual_inc: int, verification_status: string, issue_d: string, loan_status: string, pymnt_plan: string]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumn('grade', col('grade').cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|loan_amnt|   fa|\n",
      "+---------+-----+\n",
      "|    10000|10000|\n",
      "+---------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"loan_amnt\", col(\"funded_amnt\").alias(\"fa\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter & where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|loan_amnt|funded_amnt|funded_amnt_inv|      term|int_rate|installment|grade|sub_grade|           emp_title|emp_length|home_ownership|annual_inc|verification_status|issue_d|loan_status|pymnt_plan|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|    16000|      16000|          16000| 60 months|  17.97%|     406.04|    D|       D1|Instructional Coo...|   5 years|      MORTGAGE|     51000|       Not Verified| 18-Dec|    Current|         n|\n",
      "|     3500|       3500|           3500| 36 months|  20.89%|     131.67|    D|       D4|       gas attendant| 10+ years|      MORTGAGE|     40000|    Source Verified| 18-Dec|    Current|         n|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"grade\")==\"D\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|loan_amnt|funded_amnt|funded_amnt_inv|      term|int_rate|installment|grade|sub_grade|           emp_title|emp_length|home_ownership|annual_inc|verification_status|issue_d|loan_status|pymnt_plan|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|    16000|      16000|          16000| 60 months|  17.97%|     406.04|    D|       D1|Instructional Coo...|   5 years|      MORTGAGE|     51000|       Not Verified| 18-Dec|    Current|         n|\n",
      "|     3500|       3500|           3500| 36 months|  20.89%|     131.67|    D|       D4|       gas attendant| 10+ years|      MORTGAGE|     40000|    Source Verified| 18-Dec|    Current|         n|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"grade\")==\"D\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|loan_amnt|funded_amnt|funded_amnt_inv|      term|int_rate|installment|grade|sub_grade|           emp_title|emp_length|home_ownership|annual_inc|verification_status|issue_d|loan_status|pymnt_plan|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|    16000|      16000|          16000| 60 months|  17.97%|     406.04|    D|       D1|Instructional Coo...|   5 years|      MORTGAGE|     51000|       Not Verified| 18-Dec|    Current|         n|\n",
      "|    30000|      30000|          30000| 60 months|  18.94%|     777.23|    D|       D2|         Postmaster | 10+ years|      MORTGAGE|     90000|    Source Verified| 18-Dec|    Current|         n|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((col(\"grade\")==\"D\") & (col(\"loan_amnt\")>10000)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|loan_amnt|funded_amnt|funded_amnt_inv|      term|int_rate|installment|grade|sub_grade|           emp_title|emp_length|home_ownership|annual_inc|verification_status|issue_d|loan_status|pymnt_plan|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|    16000|      16000|          16000| 60 months|  17.97%|     406.04|    D|       D1|Instructional Coo...|   5 years|      MORTGAGE|     51000|       Not Verified| 18-Dec|    Current|         n|\n",
      "|    30000|      30000|          30000| 60 months|  18.94%|     777.23|    D|       D2|         Postmaster | 10+ years|      MORTGAGE|     90000|    Source Verified| 18-Dec|    Current|         n|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+--------------------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where((col(\"grade\")==\"D\") & (col(\"loan_amnt\")>10000)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|loan_amnt|funded_amnt|funded_amnt_inv|      term|int_rate|installment|grade|sub_grade|emp_title|emp_length|home_ownership|annual_inc|verification_status|issue_d|loan_status|pymnt_plan|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|    10000|      10000|          10000| 36 months|  10.33%|     324.23|    B|       B1|     null|  < 1 year|      MORTGAGE|    280000|       Not Verified| 18-Dec|    Current|         n|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"grade\").isin([\"A\",\"B\"])).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|loan_amnt|funded_amnt|funded_amnt_inv|      term|int_rate|installment|grade|sub_grade|emp_title|emp_length|home_ownership|annual_inc|verification_status|issue_d|loan_status|pymnt_plan|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|    10000|      10000|          10000| 36 months|  10.33%|     324.23|    B|       B1|     null|  < 1 year|      MORTGAGE|    280000|       Not Verified| 18-Dec|    Current|         n|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"grade\").isin([\"A\",\"B\"])).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|grade|sum(loan_amnt)|\n",
      "+-----+--------------+\n",
      "|    E|        132350|\n",
      "|    B|        478550|\n",
      "|    D|        313025|\n",
      "|    C|        551400|\n",
      "|    A|         45000|\n",
      "+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"grade\").sum('loan_amnt').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|grade|sum_LA|\n",
      "+-----+------+\n",
      "|    A| 45000|\n",
      "|    E|132350|\n",
      "|    D|313025|\n",
      "|    B|478550|\n",
      "|    C|551400|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"grade\").agg(sum('loan_amnt').alias('sum_LA')).sort(col('sum_LA').asc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge/Join DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_as1 = df.alias(\"df_as1\")\n",
    "df_as2 = df.alias(\"df_as2\")\n",
    "joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
    "joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\").sort(desc(\"df_as1.name\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_as1.union(df_as2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Function (When.Otherwise )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|loan_amnt|loan_level|\n",
      "+---------+----------+\n",
      "|    10000|      High|\n",
      "|     2500|      High|\n",
      "|    12000|      High|\n",
      "|    15000|      High|\n",
      "|    16000|      High|\n",
      "+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Multiple conditions on multiple columns\n",
    "\n",
    "df.withColumn(\"loan_level\", when(col(\"loan_amnt\") < 1000, \"Low\")\\\n",
    "                            .when((col(\"loan_amnt\") > 1000) & (col(\"funded_amnt\") > 20000), \"Mid\")\\\n",
    "                            .otherwise(\"High\"))\\\n",
    "                            .select(\"loan_amnt\",\"loan_level\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loan_amnt',\n",
       " 'funded_amnt',\n",
       " 'funded_amnt_inv',\n",
       " 'term',\n",
       " 'int_rate',\n",
       " 'installment',\n",
       " 'grade',\n",
       " 'sub_grade',\n",
       " 'emp_title',\n",
       " 'emp_length',\n",
       " 'home_ownership',\n",
       " 'annual_inc',\n",
       " 'verification_status',\n",
       " 'issue_d',\n",
       " 'loan_status',\n",
       " 'pymnt_plan']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loan_amnt',\n",
       " 'funded_amnt',\n",
       " 'funded_amnt_inv',\n",
       " 'term',\n",
       " 'int_rate',\n",
       " 'installment',\n",
       " 'grade',\n",
       " 'sub_grade',\n",
       " 'emp_title',\n",
       " 'emp_length',\n",
       " 'home_ownership',\n",
       " 'annual_inc',\n",
       " 'verification_status',\n",
       " 'issue_d',\n",
       " 'loan_status',\n",
       " 'pymnt_plan']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+\n",
      "|grade| 36 months| 60 months|\n",
      "+-----+----------+----------+\n",
      "|    E|     52350|     80000|\n",
      "|    B|    194575|    283975|\n",
      "|    D|     47950|    265075|\n",
      "|    C|    248000|    303400|\n",
      "|    A|     45000|      null|\n",
      "+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('grade').pivot('term').sum('loan_amnt').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>term</th>\n",
       "      <th>36 months</th>\n",
       "      <th>60 months</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grade</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>45000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>194575.0</td>\n",
       "      <td>283975.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>248000.0</td>\n",
       "      <td>303400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D</th>\n",
       "      <td>47950.0</td>\n",
       "      <td>265075.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>52350.0</td>\n",
       "      <td>80000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "term    36 months   60 months\n",
       "grade                        \n",
       "A         45000.0         NaN\n",
       "B        194575.0    283975.0\n",
       "C        248000.0    303400.0\n",
       "D         47950.0    265075.0\n",
       "E         52350.0     80000.0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_p = df.toPandas()\n",
    "df_p.groupby([\"grade\",\"term\"])[\"loan_amnt\"].sum().unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Support\n",
    "First register your DF as a table, If we exit, the table will be lost. To save permanently, use save options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a temporory table, result is a DF\n",
    "df1.createOrReplaceTempView('table_name')\n",
    "df2 = spark.sql('select * from tablename')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- member_id: string (nullable = true)\n",
      " |-- loan_amnt: integer (nullable = true)\n",
      " |-- funded_amnt: integer (nullable = true)\n",
      " |-- funded_amnt_inv: double (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- int_rate: string (nullable = true)\n",
      " |-- installment: double (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- sub_grade: string (nullable = true)\n",
      " |-- emp_title: string (nullable = true)\n",
      " |-- emp_length: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_inc: double (nullable = true)\n",
      " |-- verification_status: string (nullable = true)\n",
      " |-- issue_d: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- pymnt_plan: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- desc: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      " |-- addr_state: string (nullable = true)\n",
      " |-- dti: double (nullable = true)\n",
      " |-- delinq_2yrs: integer (nullable = true)\n",
      " |-- earliest_cr_line: string (nullable = true)\n",
      " |-- inq_last_6mths: integer (nullable = true)\n",
      " |-- mths_since_last_delinq: integer (nullable = true)\n",
      " |-- mths_since_last_record: integer (nullable = true)\n",
      " |-- open_acc: integer (nullable = true)\n",
      " |-- pub_rec: integer (nullable = true)\n",
      " |-- revol_bal: integer (nullable = true)\n",
      " |-- revol_util: string (nullable = true)\n",
      " |-- total_acc: integer (nullable = true)\n",
      " |-- initial_list_status: string (nullable = true)\n",
      " |-- out_prncp: double (nullable = true)\n",
      " |-- out_prncp_inv: double (nullable = true)\n",
      " |-- total_pymnt: double (nullable = true)\n",
      " |-- total_pymnt_inv: double (nullable = true)\n",
      " |-- total_rec_prncp: double (nullable = true)\n",
      " |-- total_rec_int: double (nullable = true)\n",
      " |-- total_rec_late_fee: double (nullable = true)\n",
      " |-- recoveries: double (nullable = true)\n",
      " |-- collection_recovery_fee: double (nullable = true)\n",
      " |-- last_pymnt_d: string (nullable = true)\n",
      " |-- last_pymnt_amnt: double (nullable = true)\n",
      " |-- next_pymnt_d: string (nullable = true)\n",
      " |-- last_credit_pull_d: string (nullable = true)\n",
      " |-- collections_12_mths_ex_med: integer (nullable = true)\n",
      " |-- mths_since_last_major_derog: integer (nullable = true)\n",
      " |-- policy_code: integer (nullable = true)\n",
      " |-- application_type: string (nullable = true)\n",
      " |-- annual_inc_joint: double (nullable = true)\n",
      " |-- dti_joint: double (nullable = true)\n",
      " |-- verification_status_joint: string (nullable = true)\n",
      " |-- acc_now_delinq: integer (nullable = true)\n",
      " |-- tot_coll_amt: integer (nullable = true)\n",
      " |-- tot_cur_bal: integer (nullable = true)\n",
      " |-- open_acc_6m: integer (nullable = true)\n",
      " |-- open_act_il: integer (nullable = true)\n",
      " |-- open_il_12m: integer (nullable = true)\n",
      " |-- open_il_24m: integer (nullable = true)\n",
      " |-- mths_since_rcnt_il: integer (nullable = true)\n",
      " |-- total_bal_il: integer (nullable = true)\n",
      " |-- il_util: integer (nullable = true)\n",
      " |-- open_rv_12m: integer (nullable = true)\n",
      " |-- open_rv_24m: integer (nullable = true)\n",
      " |-- max_bal_bc: integer (nullable = true)\n",
      " |-- all_util: integer (nullable = true)\n",
      " |-- total_rev_hi_lim: integer (nullable = true)\n",
      " |-- inq_fi: integer (nullable = true)\n",
      " |-- total_cu_tl: integer (nullable = true)\n",
      " |-- inq_last_12m: integer (nullable = true)\n",
      " |-- acc_open_past_24mths: integer (nullable = true)\n",
      " |-- avg_cur_bal: integer (nullable = true)\n",
      " |-- bc_open_to_buy: integer (nullable = true)\n",
      " |-- bc_util: double (nullable = true)\n",
      " |-- chargeoff_within_12_mths: integer (nullable = true)\n",
      " |-- delinq_amnt: integer (nullable = true)\n",
      " |-- mo_sin_old_il_acct: integer (nullable = true)\n",
      " |-- mo_sin_old_rev_tl_op: integer (nullable = true)\n",
      " |-- mo_sin_rcnt_rev_tl_op: integer (nullable = true)\n",
      " |-- mo_sin_rcnt_tl: integer (nullable = true)\n",
      " |-- mort_acc: integer (nullable = true)\n",
      " |-- mths_since_recent_bc: integer (nullable = true)\n",
      " |-- mths_since_recent_bc_dlq: integer (nullable = true)\n",
      " |-- mths_since_recent_inq: integer (nullable = true)\n",
      " |-- mths_since_recent_revol_delinq: integer (nullable = true)\n",
      " |-- num_accts_ever_120_pd: integer (nullable = true)\n",
      " |-- num_actv_bc_tl: integer (nullable = true)\n",
      " |-- num_actv_rev_tl: integer (nullable = true)\n",
      " |-- num_bc_sats: integer (nullable = true)\n",
      " |-- num_bc_tl: integer (nullable = true)\n",
      " |-- num_il_tl: integer (nullable = true)\n",
      " |-- num_op_rev_tl: integer (nullable = true)\n",
      " |-- num_rev_accts: integer (nullable = true)\n",
      " |-- num_rev_tl_bal_gt_0: integer (nullable = true)\n",
      " |-- num_sats: integer (nullable = true)\n",
      " |-- num_tl_120dpd_2m: integer (nullable = true)\n",
      " |-- num_tl_30dpd: integer (nullable = true)\n",
      " |-- num_tl_90g_dpd_24m: integer (nullable = true)\n",
      " |-- num_tl_op_past_12m: integer (nullable = true)\n",
      " |-- pct_tl_nvr_dlq: double (nullable = true)\n",
      " |-- percent_bc_gt_75: double (nullable = true)\n",
      " |-- pub_rec_bankruptcies: integer (nullable = true)\n",
      " |-- tax_liens: integer (nullable = true)\n",
      " |-- tot_hi_cred_lim: integer (nullable = true)\n",
      " |-- total_bal_ex_mort: integer (nullable = true)\n",
      " |-- total_bc_limit: integer (nullable = true)\n",
      " |-- total_il_high_credit_limit: integer (nullable = true)\n",
      " |-- revol_bal_joint: integer (nullable = true)\n",
      " |-- sec_app_earliest_cr_line: string (nullable = true)\n",
      " |-- sec_app_inq_last_6mths: integer (nullable = true)\n",
      " |-- sec_app_mort_acc: integer (nullable = true)\n",
      " |-- sec_app_open_acc: integer (nullable = true)\n",
      " |-- sec_app_revol_util: double (nullable = true)\n",
      " |-- sec_app_open_act_il: integer (nullable = true)\n",
      " |-- sec_app_num_rev_accts: integer (nullable = true)\n",
      " |-- sec_app_chargeoff_within_12_mths: integer (nullable = true)\n",
      " |-- sec_app_collections_12_mths_ex_med: integer (nullable = true)\n",
      " |-- sec_app_mths_since_last_major_derog: integer (nullable = true)\n",
      " |-- hardship_flag: string (nullable = true)\n",
      " |-- hardship_type: string (nullable = true)\n",
      " |-- hardship_reason: string (nullable = true)\n",
      " |-- hardship_status: string (nullable = true)\n",
      " |-- deferral_term: integer (nullable = true)\n",
      " |-- hardship_amount: double (nullable = true)\n",
      " |-- hardship_start_date: string (nullable = true)\n",
      " |-- hardship_end_date: string (nullable = true)\n",
      " |-- payment_plan_start_date: string (nullable = true)\n",
      " |-- hardship_length: integer (nullable = true)\n",
      " |-- hardship_dpd: integer (nullable = true)\n",
      " |-- hardship_loan_status: string (nullable = true)\n",
      " |-- orig_projected_additional_accrued_interest: double (nullable = true)\n",
      " |-- hardship_payoff_balance_amount: double (nullable = true)\n",
      " |-- hardship_last_payment_amount: double (nullable = true)\n",
      " |-- disbursement_method: string (nullable = true)\n",
      " |-- debt_settlement_flag: string (nullable = true)\n",
      " |-- debt_settlement_flag_date: string (nullable = true)\n",
      " |-- settlement_status: string (nullable = true)\n",
      " |-- settlement_date: string (nullable = true)\n",
      " |-- settlement_amount: integer (nullable = true)\n",
      " |-- settlement_percentage: double (nullable = true)\n",
      " |-- settlement_term: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|hardship_start_date|\n",
      "+-------------------+\n",
      "|             Feb-19|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('hardship_start_date').filter(df.hardship_start_date == 'Feb-19').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+\n",
      "|hardship_start_date|settlement_TS|\n",
      "+-------------------+-------------+\n",
      "|               null|         null|\n",
      "|             Feb-19|         null|\n",
      "+-------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('hardship_start_date', to_date(df.hardship_start_date, 'mm-yy').alias('settlement_TS')).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "windowSpec = Window.partitionBy(\"grade\").orderBy(col(\"loan_amnt\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----+\n",
      "|grade|loan_amnt|rank|\n",
      "+-----+---------+----+\n",
      "|    A|    18000|   2|\n",
      "|    A|    20000|   1|\n",
      "|    B|    35000|   2|\n",
      "|    B|    40000|   1|\n",
      "|    C|    40000|   1|\n",
      "|    C|    35000|   2|\n",
      "|    D|    35000|   1|\n",
      "|    D|    35000|   1|\n",
      "|    D|    34575|   2|\n",
      "|    D|    35000|   1|\n",
      "|    E|    30000|   1|\n",
      "|    E|    24000|   2|\n",
      "+-----+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"grade\",\"loan_amnt\").withColumn(\"rank\", dense_rank().over(windowSpec)).sort(asc(\"grade\")).filter(col(\"rank\")<3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|loan_amnt|funded_amnt|funded_amnt_inv|      term|int_rate|installment|grade|sub_grade|emp_title|emp_length|home_ownership|annual_inc|verification_status|issue_d|loan_status|pymnt_plan|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "|    10000|      10000|          10000| 36 months|  10.33%|     324.23|    B|       B1|     null|  < 1 year|      MORTGAGE|    280000|       Not Verified| 18-Dec|    Current|         n|\n",
      "+---------+-----------+---------------+----------+--------+-----------+-----+---------+---------+----------+--------------+----------+-------------------+-------+-----------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDF (User Defined Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowr = udf(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory, Caching, Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.cacheTable(\"tableName\")\n",
    "dataFrame.cache()\n",
    "spark.catalog.uncacheTable(\"tableName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are 8 partitions \n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(6).createOrReplaceTempView(\"df_repartitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.cacheTable(\"df_repartitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|hardship_start_date|\n",
      "+-------------------+\n",
      "|               null|\n",
      "|             Feb-19|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select distinct hardship_start_date from df_repartitioned').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.table(\"df_repartitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128416"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.isCached(\"df_repartitioned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. It will read teh data from S3 or fro many source, cache that into the memory for further use by reducing the size significantly\n",
    "\n",
    "2. Repartitiong helps us to reduce the DF size which is in GBs to MBs, because it compresses them columnar wise. \n",
    "\n",
    "3. Try to have each partition size to be in between 50-200MB, dont have multi KB or multi GB partition for better performance\n",
    "\n",
    "Reference - https://youtu.be/K14plpZgy_c?list=PLr-OUJWJfNGEggwo6Vt5LZGRMR1yNyUvg&t=3399"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coalesce\n",
    "\n",
    "Returns a new DataFrame that has exactly numPartitions partitions.\n",
    "\n",
    "Similar to coalesce defined on an RDD, this operation results in a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions. If a larger number of partitions is requested, it will stay at the current number of partitions.\n",
    "\n",
    "However, if you’re doing a drastic coalesce, e.g. to numPartitions = 1, this may result in your computation taking place on fewer nodes than you like (e.g. one node in the case of numPartitions = 1). To avoid this, you can call repartition(). This will add a shuffle step, but means the current upstream partitions will be executed in parallel (per whatever the current partitioning is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.coalesce(1).rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format('parquet').save('/folder/df_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|grade|\n",
      "+-----+\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "|    A|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.filter(\"grade='A'\").select(\"grade\").show()\n",
    "tw_filter = df.filter(df.grade=='A').select(\"grade\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'explain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-21924ce81365>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtw_filter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'explain'"
     ]
    }
   ],
   "source": [
    "tw_filter.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "| _c0|      _c1|\n",
      "+----+---------+\n",
      "|  id|member_id|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "|null|     null|\n",
      "+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run SQL on files directly\n",
    "spark.sql(\"select _c0, _c1 from csv. `LoanStats_2018Q4.csv`\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in df.columns:\n",
    "#     df = df.withColumnRenamed(c, c.replace(\" \",\"_\")) \\\n",
    "#         .withColumnRenamed(c, c.replace(\".\",\"\"))\n",
    "        \n",
    "# df = df.withColumnRenamed(\"Engine_(ltr)\", \"Engine_ltr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|disbursement_method|\n",
      "+-------------------+\n",
      "|          DirectPay|\n",
      "|               Cash|\n",
      "|               Cash|\n",
      "|               Cash|\n",
      "|               Cash|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select disbursement_method from loan_status\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|addr_state|avg_income|\n",
      "+----------+----------+\n",
      "|        SC|      1692|\n",
      "|        AZ|      3149|\n",
      "|        LA|      1328|\n",
      "|        MN|      2078|\n",
      "|        NJ|      4711|\n",
      "|        DC|       274|\n",
      "|        OR|      1562|\n",
      "|        VA|      3364|\n",
      "|      null|         0|\n",
      "|        RI|       614|\n",
      "|        KY|      1280|\n",
      "|        WY|       241|\n",
      "|        NH|       643|\n",
      "|        MI|      3249|\n",
      "|        NV|      2006|\n",
      "|        WI|      1729|\n",
      "|        ID|       501|\n",
      "|        CA|     17879|\n",
      "|        NE|       642|\n",
      "|        CT|      2038|\n",
      "+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"addr_state\")\\\n",
    "    .agg({\"addr_state\":'count'})\\\n",
    "    .withColumnRenamed(\"count(addr_state)\", \"avg_income\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acc_now_delinq',\n",
       " 'acc_open_past_24mths',\n",
       " 'addr_state',\n",
       " 'all_util',\n",
       " 'annual_inc',\n",
       " 'annual_inc_joint',\n",
       " 'application_type',\n",
       " 'avg_cur_bal',\n",
       " 'bc_open_to_buy',\n",
       " 'bc_util',\n",
       " 'chargeoff_within_12_mths',\n",
       " 'collection_recovery_fee',\n",
       " 'collections_12_mths_ex_med',\n",
       " 'debt_settlement_flag',\n",
       " 'debt_settlement_flag_date',\n",
       " 'deferral_term',\n",
       " 'delinq_2yrs',\n",
       " 'delinq_amnt',\n",
       " 'desc',\n",
       " 'disbursement_method',\n",
       " 'dti',\n",
       " 'dti_joint',\n",
       " 'earliest_cr_line',\n",
       " 'emp_length',\n",
       " 'emp_title',\n",
       " 'funded_amnt',\n",
       " 'funded_amnt_inv',\n",
       " 'grade',\n",
       " 'hardship_amount',\n",
       " 'hardship_dpd',\n",
       " 'hardship_end_date',\n",
       " 'hardship_flag',\n",
       " 'hardship_last_payment_amount',\n",
       " 'hardship_length',\n",
       " 'hardship_loan_status',\n",
       " 'hardship_payoff_balance_amount',\n",
       " 'hardship_reason',\n",
       " 'hardship_start_date',\n",
       " 'hardship_status',\n",
       " 'hardship_type',\n",
       " 'home_ownership',\n",
       " 'id',\n",
       " 'il_util',\n",
       " 'initial_list_status',\n",
       " 'inq_fi',\n",
       " 'inq_last_12m',\n",
       " 'inq_last_6mths',\n",
       " 'installment',\n",
       " 'int_rate',\n",
       " 'issue_d',\n",
       " 'last_credit_pull_d',\n",
       " 'last_pymnt_amnt',\n",
       " 'last_pymnt_d',\n",
       " 'loan_amnt',\n",
       " 'loan_status',\n",
       " 'max_bal_bc',\n",
       " 'member_id',\n",
       " 'mo_sin_old_il_acct',\n",
       " 'mo_sin_old_rev_tl_op',\n",
       " 'mo_sin_rcnt_rev_tl_op',\n",
       " 'mo_sin_rcnt_tl',\n",
       " 'mort_acc',\n",
       " 'mths_since_last_delinq',\n",
       " 'mths_since_last_major_derog',\n",
       " 'mths_since_last_record',\n",
       " 'mths_since_rcnt_il',\n",
       " 'mths_since_recent_bc',\n",
       " 'mths_since_recent_bc_dlq',\n",
       " 'mths_since_recent_inq',\n",
       " 'mths_since_recent_revol_delinq',\n",
       " 'next_pymnt_d',\n",
       " 'num_accts_ever_120_pd',\n",
       " 'num_actv_bc_tl',\n",
       " 'num_actv_rev_tl',\n",
       " 'num_bc_sats',\n",
       " 'num_bc_tl',\n",
       " 'num_il_tl',\n",
       " 'num_op_rev_tl',\n",
       " 'num_rev_accts',\n",
       " 'num_rev_tl_bal_gt_0',\n",
       " 'num_sats',\n",
       " 'num_tl_120dpd_2m',\n",
       " 'num_tl_30dpd',\n",
       " 'num_tl_90g_dpd_24m',\n",
       " 'num_tl_op_past_12m',\n",
       " 'open_acc',\n",
       " 'open_acc_6m',\n",
       " 'open_act_il',\n",
       " 'open_il_12m',\n",
       " 'open_il_24m',\n",
       " 'open_rv_12m',\n",
       " 'open_rv_24m',\n",
       " 'orig_projected_additional_accrued_interest',\n",
       " 'out_prncp',\n",
       " 'out_prncp_inv',\n",
       " 'payment_plan_start_date',\n",
       " 'pct_tl_nvr_dlq',\n",
       " 'percent_bc_gt_75',\n",
       " 'policy_code',\n",
       " 'pub_rec',\n",
       " 'pub_rec_bankruptcies',\n",
       " 'purpose',\n",
       " 'pymnt_plan',\n",
       " 'recoveries',\n",
       " 'revol_bal',\n",
       " 'revol_bal_joint',\n",
       " 'revol_util',\n",
       " 'sec_app_chargeoff_within_12_mths',\n",
       " 'sec_app_collections_12_mths_ex_med',\n",
       " 'sec_app_earliest_cr_line',\n",
       " 'sec_app_inq_last_6mths',\n",
       " 'sec_app_mort_acc',\n",
       " 'sec_app_mths_since_last_major_derog',\n",
       " 'sec_app_num_rev_accts',\n",
       " 'sec_app_open_acc',\n",
       " 'sec_app_open_act_il',\n",
       " 'sec_app_revol_util',\n",
       " 'settlement_amount',\n",
       " 'settlement_date',\n",
       " 'settlement_percentage',\n",
       " 'settlement_status',\n",
       " 'settlement_term',\n",
       " 'sub_grade',\n",
       " 'tax_liens',\n",
       " 'term',\n",
       " 'title',\n",
       " 'tot_coll_amt',\n",
       " 'tot_cur_bal',\n",
       " 'tot_hi_cred_lim',\n",
       " 'total_acc',\n",
       " 'total_bal_ex_mort',\n",
       " 'total_bal_il',\n",
       " 'total_bc_limit',\n",
       " 'total_cu_tl',\n",
       " 'total_il_high_credit_limit',\n",
       " 'total_pymnt',\n",
       " 'total_pymnt_inv',\n",
       " 'total_rec_int',\n",
       " 'total_rec_late_fee',\n",
       " 'total_rec_prncp',\n",
       " 'total_rev_hi_lim',\n",
       " 'url',\n",
       " 'verification_status',\n",
       " 'verification_status_joint',\n",
       " 'zip_code']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.38840887987951"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.cov(\"Registration_Year\", \"Scrappage_volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10713454005480579"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.corr(\"Registration_Year\", \"Scrappage_volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|         Make| count|\n",
      "+-------------+------+\n",
      "|    CHEVROLET|127335|\n",
      "|         FORD|115475|\n",
      "|          GMC| 81739|\n",
      "|        DODGE| 72821|\n",
      "|MERCEDES-BENZ| 47042|\n",
      "|       TOYOTA| 42135|\n",
      "|          BMW| 31962|\n",
      "|       NISSAN| 31257|\n",
      "|      PONTIAC| 30856|\n",
      "|   VOLKSWAGEN| 28435|\n",
      "|         AUDI| 26378|\n",
      "|   OLDSMOBILE| 24273|\n",
      "|     CHRYSLER| 23933|\n",
      "|        BUICK| 23195|\n",
      "|     CADILLAC| 21132|\n",
      "|        VOLVO| 20666|\n",
      "|        MAZDA| 20378|\n",
      "|         JEEP| 15717|\n",
      "|        HONDA| 15717|\n",
      "|      HYUNDAI| 15366|\n",
      "+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Make\").count().orderBy('count',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.where(df.Make.isin([\"BMW\", \"VOLVO\"])).show()\n",
    "\n",
    "# df.filter(df.Make == \"BMW\").show()\n",
    "# df.groupBy(\"Make\",\"Scrappage_volume\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## regexp_replace, regexp_extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leetcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/c:/Users/akash.mathur/Downloads/Akash/Spark/Book1.csv",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-a470f735ce2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#     .load(file_location)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     .options(inferSchema= infer_schema,\n\u001b[0;32m     16\u001b[0m     \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mfirst_row_is_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Scrappage\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Scrappage\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Scrappage\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Path does not exist: file:/c:/Users/akash.mathur/Downloads/Akash/Spark/Book1.csv"
     ]
    }
   ],
   "source": [
    "file_location = \"c://Users//akash.mathur//Downloads//Akash//Spark/Book1.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# df = spark.read.format('csv')\\\n",
    "#     .option(\"inferSchema\", infer_schema)\\\n",
    "#     .option(\"header\", first_row_is_header)\\\n",
    "#     .option(\"sep\", delimiter)\\\n",
    "#     .load(file_location)\n",
    "    \n",
    "df = spark.read.format('csv')\\\n",
    "    .options(inferSchema= infer_schema,\n",
    "    header= first_row_is_header,\n",
    "    sep= delimiter)\\\n",
    "    .load(file_location)\n",
    "\n",
    "# ALTERNATE\n",
    "# spark.read.load(\"LoanStats_2018Q4.csv\",\n",
    "#                      format=\"csv\", sep=\";\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "# spark.read.csv(\"LoanStats_2018Q4.csv\",\n",
    "#                      inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = [\n",
    "#     (1,1,1),\n",
    "#     (1,1,1),\n",
    "#     (1,1,2),\n",
    "#     (1,2,3),\n",
    "#     (1,2,4),\n",
    "#     (2,1,5),\n",
    "#     (2,1,6)\n",
    "#     ]\n",
    "\n",
    "data = [[\"node.js\", \"dbms\", \"integration\"],\n",
    "        [\"jsp\", \"SQL\", \"trigonometry\"],\n",
    "        [\"php\", \"oracle\", \"statistics\"],\n",
    "        [\".net\", \"db2\", \"Machine Learning\"]]\n",
    "\n",
    "columns=['actor_id','director_id','timestamp']\n",
    "\n",
    "sdf = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o137.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (INGGNLT1PYD593.INFO.CORP executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 29 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-dfaa919e6fc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\Scrappage\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \"\"\"\n\u001b[0;32m    483\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Scrappage\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Scrappage\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Scrappage\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o137.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7.0 failed 1 times, most recent failure: Lost task 0.0 in stage 7.0 (TID 7) (INGGNLT1PYD593.INFO.CORP executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 29 more\r\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sdf.withColumn('actor_id', col('actor_id').cast(\"Integer\"))\\\n",
    "#     .withColumn('director_id', col('director_id').cast(\"Integer\"))\\\n",
    "#     .withColumn('timestamp', col('timestamp').cast(\"Integer\"))\\\n",
    "#     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- actor_id: long (nullable = true)\n",
      " |-- director_id: long (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "germany = spark.read.csv('germany_newreg.csv', inferSchema=True, header=True)\n",
    "df = germany.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toDF(*[re.sub(\"[^a-zA-Z0-9]\",\"\",column) for column in df.columns])\n",
    "\n",
    "used_cols = [\"Make\",\"RegistrationType\",\"Model\",\"SubModelShort\",\"BodyType\",\"FuelType\",\"DrivenWheels\",\"NoofDoors\",\"Turbo\",\"Engineltr\",\"Engineccm\",\"EnginekW\",\n",
    "                 \"GrossVehicleWeight\",\"NoofCylinders\",\"1980\",\"1981\",\"1982\",\"1983\",\"1984\",\"1985\",\"1986\",\"1987\",\"1988\",\"1989\",\"1990\",\"1991\",\"1992\",\"1993\",\"1994\",\"1995\",\"1996\",\"1997\",\n",
    "                 \"1998\",\"1999\",\"2000\",\"2001\",\"2002\",\"2003\",\"2004\",\"2005\",\"2006\",\"2007\",\"2008\",\"2009\",\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\",\"2021\"]\n",
    "\n",
    "df = df.select(used_cols)\n",
    "df = df.filter(df['RegistrationType'] != \"Heavy Commercial Vehicles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def melt_df(\n",
    "        df: DataFrame,\n",
    "        id_vars: str, value_vars: str,\n",
    "        var_name: str=\"variable\", value_name: str=\"value\") -> DataFrame:\n",
    "    \"\"\"Convert :class:`DataFrame` from wide to long format.\"\"\"\n",
    "\n",
    "    # Create array<struct<variable: str, value: ...>>\n",
    "    _vars_and_vals = array(*(\n",
    "        struct(lit(c).alias(var_name), col(c).alias(value_name))\n",
    "        for c in value_vars))\n",
    "\n",
    "    # Add to the DataFrame and explode\n",
    "    _tmp = df.withColumn(\"_vars_and_vals\", explode(_vars_and_vals))\n",
    "\n",
    "    cols = id_vars + [\n",
    "            col(\"_vars_and_vals\")[x].alias(x) for x in [var_name, value_name]]\n",
    "    return _tmp.select(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = melt_df(df=df, \n",
    "        id_vars=[\"Make\",\"RegistrationType\",\"Model\",\"SubModelShort\",\"BodyType\",\"FuelType\",\"DrivenWheels\",\"NoofDoors\",\"Turbo\",\"Engineltr\",\"Engineccm\",\"EnginekW\",\n",
    "                 \"GrossVehicleWeight\",\"NoofCylinders\"],\n",
    "        value_vars=[\"1980\",\"1981\",\"1982\",\"1983\",\"1984\",\"1985\",\"1986\",\"1987\",\"1988\",\"1989\",\"1990\",\"1991\",\"1992\",\"1993\",\"1994\",\"1995\",\"1996\",\"1997\",\n",
    "                 \"1998\",\"1999\",\"2000\",\"2001\",\"2002\",\"2003\",\"2004\",\"2005\",\"2006\",\"2007\",\"2008\",\"2009\",\"2010\",\"2011\",\"2012\",\"2013\",\"2014\",\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\",\"2021\"],\n",
    "        \n",
    "        var_name = 'year',\n",
    "        value_name = 'sales')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cols = [item[0] for item in df1.dtypes if item[1]=='string']\n",
    "non_string_cols = [item[0] for item in df1.dtypes if item[1]!='string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.select(*[F.upper(c).alias(c) for c in string_cols], *non_string_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.withColumn('year', df1['year'].cast(IntegerType()))\n",
    "df1 = df1.filter(df1.sales>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-column-functions/\n",
    "df1.EnginekW = df1.EnginekW.astype('int')\n",
    "df1.Engineltr = df1.Engineltr.cast('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Make: string (nullable = true)\n",
      " |-- RegistrationType: string (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- SubModelShort: string (nullable = true)\n",
      " |-- BodyType: string (nullable = true)\n",
      " |-- FuelType: string (nullable = true)\n",
      " |-- DrivenWheels: string (nullable = true)\n",
      " |-- Turbo: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- NoofDoors: integer (nullable = true)\n",
      " |-- Engineltr: double (nullable = true)\n",
      " |-- Engineccm: integer (nullable = true)\n",
      " |-- EnginekW: integer (nullable = true)\n",
      " |-- GrossVehicleWeight: integer (nullable = true)\n",
      " |-- NoofCylinders: integer (nullable = true)\n",
      " |-- sales: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(sales)|\n",
      "+----------+\n",
      "| 134926194|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.agg({'sales':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|year|sum(sales)|\n",
      "+----+----------+\n",
      "|1980|         0|\n",
      "|1981|   2330333|\n",
      "|1982|   2155537|\n",
      "|1983|   2426774|\n",
      "|1984|   2393939|\n",
      "|1985|   2379261|\n",
      "|1986|   2829438|\n",
      "|1987|   2915654|\n",
      "|1988|   2807939|\n",
      "|1989|   2831740|\n",
      "|1990|   3158581|\n",
      "|1991|   4358648|\n",
      "|1992|   4140027|\n",
      "|1993|   3360786|\n",
      "|1994|   3383613|\n",
      "|1995|   3483517|\n",
      "|1996|   3659240|\n",
      "|1997|   3702780|\n",
      "|1998|   3927768|\n",
      "|1999|   4008736|\n",
      "|2000|   3579661|\n",
      "|2001|   3536380|\n",
      "|2002|   3435717|\n",
      "|2003|   3413622|\n",
      "|2004|   3452449|\n",
      "|2005|   3517391|\n",
      "|2006|   3665154|\n",
      "|2007|   3369760|\n",
      "|2008|   3313329|\n",
      "|2009|   3976611|\n",
      "|2010|   3112815|\n",
      "|2011|   3407073|\n",
      "|2012|   3301942|\n",
      "|2013|   3165136|\n",
      "|2014|   3265110|\n",
      "|2015|   3443978|\n",
      "|2016|   3609656|\n",
      "|2017|   3711972|\n",
      "|2018|   3721125|\n",
      "|2019|   3912258|\n",
      "|2020|   3185568|\n",
      "|2021|   1575176|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupby('year').agg({'sales':'sum'}).orderBy('year', ascending=True).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doors imputation \n",
    "\n",
    "doors_dict = {'BUS CITY': 2,\n",
    " 'BUS MULTI PURPOSE': 2,\n",
    " 'BUS TOURING COACH': 2,\n",
    " 'BUS UNSPEC.': 2,\n",
    " 'CAR PICKUP': 2,\n",
    " 'CAR UNSPEC.': 4,\n",
    " 'COMMERCIAL UNSPEC.': 2,\n",
    " 'CONVERTIBLE': 2,\n",
    " 'COUPE': 2,\n",
    " 'ESTATE HIGH VOLUME': 4,\n",
    " 'HATCHBACK': 5,\n",
    " 'HCV RECREATIONAL TRUCK': 2,\n",
    " 'HCV RIGID TRUCK': 2,\n",
    " 'HCV TRACTOR TRUCK': 2,\n",
    " 'HCV TRUCK UNSPEC.': 2,\n",
    " 'LCV COMBI VAN': 4,\n",
    " 'LCV PANEL VAN': 4,\n",
    " 'LCV PANEL VAN DOUBLE CAB': 4,\n",
    " 'LCV PICKUP VAN': 4,\n",
    " 'LCV PICKUP VAN DOUBLE CAB': 4,\n",
    " 'LCV RECREATIONAL VAN': 4,\n",
    " 'LCV TIPPER': 2,\n",
    " 'LCV TRACTOR': 2,\n",
    " 'LCV TRUCK/CHASSIS CAB': 2,\n",
    " 'LCV VAN BUS': 4,\n",
    " 'LCV VAN/TRUCK UNSPEC.': 4,\n",
    " 'MONOSPACE': 4,\n",
    " 'MONOSPACE PANEL VAN': 4,\n",
    " 'MONOSPACE UNSPEC.': 4,\n",
    " 'PICKUP DOUBLE CAB': 4,\n",
    " 'PICKUP LONG CAB': 4,\n",
    " 'PICKUP SINGLE CAB': 2,\n",
    " 'PICKUP TIPPER': 2,\n",
    " 'PICKUP UNSPEC.': 2,\n",
    " 'RETRACTABLE HARDTOP': 2,\n",
    " 'ROADSTER': 2,\n",
    " 'SEDAN': 4,\n",
    " 'SUV CLOSED': 4,\n",
    " 'SUV OPEN': 4,\n",
    " 'SUV PICKUP': 4,\n",
    " 'SUV UNSPEC.': 4,\n",
    " 'TARGA/T-ROOF': 2,\n",
    " 'UNSPECIFIED': 4,\n",
    " 'WAGON': 5}\n",
    "\n",
    "# # Mapped doors based on Body type\n",
    "# germany['No. of Doors'] = germany['Body Type'].map(doors_dict)\n",
    "# # Replace -2/-1 with NaN\n",
    "# germany.replace(-2, -1, inplace=True)\n",
    "# germany.replace(-1, np.nan,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_col  = F.create_map([F.lit(x) for i in doors_dict.items() for x in i])\n",
    "df1 = df1.withColumn('NoofDoors', map_col[F.col('BodyType')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s= to_date('2022-01-28')\n",
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|collect_set(Turbo)             |\n",
      "+-------------------------------+\n",
      "|[Non turbo, Unspecified, Turbo]|\n",
      "+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(collect_set(\"Turbo\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------------+-------------+----------+---------------+------------+---------+----+---------+---------+---------+--------+------------------+-------------+-----+\n",
      "|      Make|RegistrationType|          Model|SubModelShort|  BodyType|       FuelType|DrivenWheels|    Turbo|year|NoofDoors|Engineltr|Engineccm|EnginekW|GrossVehicleWeight|NoofCylinders|sales|\n",
      "+----------+----------------+---------------+-------------+----------+---------------+------------+---------+----+---------+---------+---------+--------+------------------+-------------+-----+\n",
      "|    AIWAYS|  PASSENGER CARS|      AIWAYS U5|           U5|SUV CLOSED|ELECTRIC W/OREX|       FRONT|NON TURBO|2021|        4|      0.0|        0|     140|              2155|            0|    3|\n",
      "|    AIWAYS|  PASSENGER CARS|      AIWAYS U5|           U5|SUV CLOSED|ELECTRIC W/OREX|       FRONT|NON TURBO|2020|        4|      0.0|        0|     140|              2155|            0|    8|\n",
      "|    AIWAYS|  PASSENGER CARS|      AIWAYS U5|           U5|SUV CLOSED|ELECTRIC W/OREX|       FRONT|NON TURBO|2021|        4|      0.0|        0|     140|              2155|            0|    1|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         DIESEL|       FRONT|    TURBO|2009|        5|      1.6|     1598|      85|              1685|            4|    1|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         DIESEL|       FRONT|    TURBO|2008|        5|      1.6|     1598|      88|              1685|            4|  108|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         DIESEL|       FRONT|    TURBO|2009|        5|      1.6|     1598|      88|              1685|            4|  324|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         DIESEL|       FRONT|    TURBO|2010|        5|      1.6|     1598|      88|              1685|            4|  133|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         DIESEL|       FRONT|    TURBO|2011|        5|      1.6|     1598|      88|              1685|            4|  111|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         DIESEL|       FRONT|    TURBO|2012|        5|      1.6|     1598|      88|              1685|            4|   73|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         DIESEL|       FRONT|    TURBO|2013|        5|      1.6|     1598|      88|              1685|            4|   46|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         DIESEL|       FRONT|    TURBO|2014|        5|      1.6|     1598|      88|              1685|            4|   48|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         DIESEL|       FRONT|    TURBO|2015|        5|      1.6|     1598|      88|              1685|            4|   23|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         DIESEL|       FRONT|    TURBO|2016|        5|      1.6|     1598|      88|              1685|            4|    1|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK| LPG (PETR.GAS)|       FRONT|    TURBO|2014|        5|      1.4|     1368|     125|              1650|            4|    1|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         PETROL|       FRONT|NON TURBO|2012|        5|      1.4|     1368|      51|              1560|            4|    1|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         PETROL|       FRONT|NON TURBO|2013|        5|      1.4|     1368|      51|              1560|            4|    5|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         PETROL|       FRONT|NON TURBO|2011|        5|      1.4|     1368|      57|              1560|            4|   42|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         PETROL|       FRONT|NON TURBO|2012|        5|      1.4|     1368|      57|              1560|            4|   86|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         PETROL|       FRONT|NON TURBO|2013|        5|      1.4|     1368|      57|              1560|            4|   69|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         PETROL|       FRONT|NON TURBO|2014|        5|      1.4|     1368|      57|              1560|            4|  151|\n",
      "+----------+----------------+---------------+-------------+----------+---------------+------------+---------+----+---------+---------+---------+--------+------------------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(sales)|\n",
      "+----------+\n",
      "| 134926194|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.agg({'sales':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+-----------+--------------+--------------------+--------+------------+-----+----+---------+---------+---------+--------+------------------+-------------+-----+\n",
      "|Make|RegistrationType|      Model| SubModelShort|            BodyType|FuelType|DrivenWheels|Turbo|year|NoofDoors|Engineltr|Engineccm|EnginekW|GrossVehicleWeight|NoofCylinders|sales|\n",
      "+----+----------------+-----------+--------------+--------------------+--------+------------+-----+----+---------+---------+---------+--------+------------------+-------------+-----+\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1980|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1981|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1982|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1983|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1984|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1985|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1986|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1987|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1988|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1989|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1990|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1991|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1992|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1993|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1994|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1995|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1996|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1997|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1998|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1999|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2000|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2001|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2002|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2003|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2004|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2005|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2006|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2007|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2008|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2009|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2010|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2011|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2012|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2013|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2014|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2015|        4|      2.3|     2287|      -1|                -2|            4|    1|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2016|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2017|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2018|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2019|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2020|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2021|        4|      2.3|     2287|      -1|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 35/MAXI|LCV VAN/TRUCK UNS...|  DIESEL| UNSPECIFIED|TURBO|1980|        4|      2.3|     2287|     109|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 35/MAXI|LCV VAN/TRUCK UNS...|  DIESEL| UNSPECIFIED|TURBO|1981|        4|      2.3|     2287|     109|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 35/MAXI|LCV VAN/TRUCK UNS...|  DIESEL| UNSPECIFIED|TURBO|1982|        4|      2.3|     2287|     109|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 35/MAXI|LCV VAN/TRUCK UNS...|  DIESEL| UNSPECIFIED|TURBO|1983|        4|      2.3|     2287|     109|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 35/MAXI|LCV VAN/TRUCK UNS...|  DIESEL| UNSPECIFIED|TURBO|1984|        4|      2.3|     2287|     109|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 35/MAXI|LCV VAN/TRUCK UNS...|  DIESEL| UNSPECIFIED|TURBO|1985|        4|      2.3|     2287|     109|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 35/MAXI|LCV VAN/TRUCK UNS...|  DIESEL| UNSPECIFIED|TURBO|1986|        4|      2.3|     2287|     109|                -2|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 35/MAXI|LCV VAN/TRUCK UNS...|  DIESEL| UNSPECIFIED|TURBO|1987|        4|      2.3|     2287|     109|                -2|            4|    0|\n",
      "+----+----------------+-----------+--------------+--------------------+--------+------------+-----+----+---------+---------+---------+--------+------------------+-------------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(df1['GrossVehicleWeight']==-2).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.replace([-1,-2], [None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+-----------+--------------+--------------------+--------+------------+-----+----+---------+---------+---------+--------+------------------+-------------+-----+\n",
      "|Make|RegistrationType|      Model| SubModelShort|            BodyType|FuelType|DrivenWheels|Turbo|year|NoofDoors|Engineltr|Engineccm|EnginekW|GrossVehicleWeight|NoofCylinders|sales|\n",
      "+----+----------------+-----------+--------------+--------------------+--------+------------+-----+----+---------+---------+---------+--------+------------------+-------------+-----+\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1980|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1981|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1982|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1983|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1984|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1985|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1986|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1987|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1988|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1989|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1990|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1991|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1992|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1993|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1994|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1995|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1996|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1997|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1998|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|1999|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2000|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2001|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2002|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2003|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2004|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2005|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2006|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2007|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2008|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2009|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2010|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2011|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2012|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2013|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2014|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2015|        4|      2.3|     2287|    null|              null|            4|    1|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2016|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2017|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2018|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2019|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2020|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|LCV VAN/TRUCK UNS...|  DIESEL|       FRONT|TURBO|2021|        4|      2.3|     2287|    null|              null|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|       LCV COMBI VAN|  DIESEL|       FRONT|TURBO|1980|        4|      2.3|     2287|      83|              3500|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|       LCV COMBI VAN|  DIESEL|       FRONT|TURBO|1981|        4|      2.3|     2287|      83|              3500|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|       LCV COMBI VAN|  DIESEL|       FRONT|TURBO|1982|        4|      2.3|     2287|      83|              3500|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|       LCV COMBI VAN|  DIESEL|       FRONT|TURBO|1983|        4|      2.3|     2287|      83|              3500|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|       LCV COMBI VAN|  DIESEL|       FRONT|TURBO|1984|        4|      2.3|     2287|      83|              3500|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|       LCV COMBI VAN|  DIESEL|       FRONT|TURBO|1985|        4|      2.3|     2287|      83|              3500|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|       LCV COMBI VAN|  DIESEL|       FRONT|TURBO|1986|        4|      2.3|     2287|      83|              3500|            4|    0|\n",
      "|FIAT|  PASSENGER CARS|FIAT DUCATO|DUCATO 18/MAXI|       LCV COMBI VAN|  DIESEL|       FRONT|TURBO|1987|        4|      2.3|     2287|      83|              3500|            4|    0|\n",
      "+----+----------------+-----------+--------------+--------------------+--------+------------+-----+----+---------+---------+---------+--------+------------------+-------------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.filter((df2['Engineccm']==2287) & (df2['SubModelShort']=='DUCATO 18/MAXI')).show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Engineltr|\n",
      "+---------+\n",
      "|   165942|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(count(when(isnan('Engineltr'),'Engineltr')).alias('Engineltr')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|GrossVehicleWeight|\n",
      "+------------------+\n",
      "|            592284|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(count(when(isnan('GrossVehicleWeight')|col('GrossVehicleWeight').isNull(),'GrossVehicleWeight')).alias('GrossVehicleWeight')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+-----+-------------+--------+--------+------------+-----+----+---------+---------+---------+--------+------------------+-------------+-----+\n",
      "|Make|RegistrationType|Model|SubModelShort|BodyType|FuelType|DrivenWheels|Turbo|year|NoofDoors|Engineltr|Engineccm|EnginekW|GrossVehicleWeight|NoofCylinders|sales|\n",
      "+----+----------------+-----+-------------+--------+--------+------------+-----+----+---------+---------+---------+--------+------------------+-------------+-----+\n",
      "|   0|               0|    0|            0|       0|       0|           0|    0|   0|        0|     6295|     6256|   10980|             24222|        26298|    0|\n",
      "+----+----------------+-----+-------------+--------+--------+------------+-----+----+---------+---------+---------+--------+------------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3 = df2.select([sum(when(isnan(c) | col(c).isNull(),1).otherwise(0)).alias(c) for c in df2.columns])\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNSPECIFIED', 'NON TURBO', 'TURBO']"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turbos = df2.select('Turbo').distinct().collect()\n",
    "turbos = [turbos[e]['Turbo'] for e in range(len(turbos))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df2.select('Turbo').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNSPECIFIED'"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0].Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+-----------+---------+------+\n",
      "|           Make|               Model|UNSPECIFIED|NON TURBO| TURBO|\n",
      "+---------------+--------------------+-----------+---------+------+\n",
      "|          DACIA|       DACIA SANDERO|          2|   141126|128481|\n",
      "|   VOLVO TRUCKS|VOLVO TRUCKS FH-S...|       null|     null|     3|\n",
      "|           SEAT|          SEAT TERRA|         15|    20728|  null|\n",
      "|            BMW|              BMW X2|         64|     null| 37262|\n",
      "|            GAZ|         GAZ UNSPEC.|          1|     null|  null|\n",
      "|        CITROEN|  CITROEN C3 PICASSO|       null|    19922| 12935|\n",
      "|      SSANGYONG|     SSANGYONG MUSSO|       null|     null|   381|\n",
      "|          ROVER|            ROVER 45|       null|     4319|  1171|\n",
      "|          VOLVO|       VOLVO UNSPEC.|       1934|      391|   335|\n",
      "|         DAEWOO|      DAEWOO LACETTI|       null|     1935|  null|\n",
      "|      CHEVROLET|   CHEVROLET LACETTI|       null|     5004|   559|\n",
      "|GIOTTI VICTORIA|GIOTTI VICTORIA G...|       null|       46|  null|\n",
      "|          LEXUS|            LEXUS LS|       null|     7247|    20|\n",
      "|       GALLOPER|   GALLOPER GALLOPER|        391|     2061| 17816|\n",
      "|            BMW|     BMW 3-SERIES GT|       null|     null| 42538|\n",
      "|        HYUNDAI|      HYUNDAI LANTRA|       null|    66961|  null|\n",
      "|         NISSAN|        NISSAN NP300|       null|     null|  1304|\n",
      "|     ALFA ROMEO|       ALFA ROMEO 33|       null|    46515|     1|\n",
      "|           ASIA|         ASIA ROCSTA|       null|     1854|  null|\n",
      "|      DE TOMASO|     DE TOMASO GUARA|       null|        8|  null|\n",
      "+---------------+--------------------+-----------+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-pivot-and-unpivot-dataframe/#:~:text=PySpark%20pivot()%20function%20is,individual%20columns%20with%20distinct%20data.\n",
    "pivot_df = df2.groupBy(['Make','Model']).pivot('Turbo', turbos).sum('sales')\n",
    "pivot_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| excel|\n",
      "+------+\n",
      "|269607|\n",
      "+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivot_df.select((col('NON TURBO')+col('TURBO')).alias('excel')).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView('germany_scrappage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.groupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+---------------+-------------+----------+---------------+------------+---------+----+---------+---------+---------+--------+------------------+-------------+-----+\n",
      "|      Make|RegistrationType|          Model|SubModelShort|  BodyType|       FuelType|DrivenWheels|    Turbo|year|NoofDoors|Engineltr|Engineccm|EnginekW|GrossVehicleWeight|NoofCylinders|sales|\n",
      "+----------+----------------+---------------+-------------+----------+---------------+------------+---------+----+---------+---------+---------+--------+------------------+-------------+-----+\n",
      "|    AIWAYS|  PASSENGER CARS|      AIWAYS U5|           U5|SUV CLOSED|ELECTRIC W/OREX|       FRONT|NON TURBO|2021|        4|      0.0|        0|     140|              2155|            0|    3|\n",
      "|    AIWAYS|  PASSENGER CARS|      AIWAYS U5|           U5|SUV CLOSED|ELECTRIC W/OREX|       FRONT|NON TURBO|2020|        4|      0.0|        0|     140|              2155|            0|    8|\n",
      "|    AIWAYS|  PASSENGER CARS|      AIWAYS U5|           U5|SUV CLOSED|ELECTRIC W/OREX|       FRONT|NON TURBO|2021|        4|      0.0|        0|     140|              2155|            0|    1|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         DIESEL|       FRONT|    TURBO|2009|        5|      1.6|     1598|      85|              1685|            4|    1|\n",
      "|ALFA ROMEO|  PASSENGER CARS|ALFA ROMEO MITO|         MITO| HATCHBACK|         DIESEL|       FRONT|    TURBO|2008|        5|      1.6|     1598|      88|              1685|            4|  108|\n",
      "+----------+----------------+---------------+-------------+----------+---------------+------------+---------+----+---------+---------+---------+--------+------------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from germany_scrappage limit 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n",
      "|NoofCylinders|    CC|\n",
      "+-------------+------+\n",
      "|            4|340456|\n",
      "|            6| 52686|\n",
      "|            5| 26754|\n",
      "|            8| 18662|\n",
      "|            3| 11026|\n",
      "|            0|  3917|\n",
      "|           12|  2586|\n",
      "|            2|   766|\n",
      "|           10|   550|\n",
      "|            1|   178|\n",
      "|           16|    43|\n",
      "|         null|     0|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select NoofCylinders, count(NoofCylinders) as CC from germany_scrappage group by NoofCylinders order by CC DESC').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------------------+------------------+--------+--------------------+-------------+---+\n",
      "|year|      Make|               Model|     SubModelShort|FuelType|            BodyType|NoofCylinders| CC|\n",
      "+----+----------+--------------------+------------------+--------+--------------------+-------------+---+\n",
      "|2005|      FIAT|         FIAT DUCATO|    DUCATO 18/MAXI|  DIESEL|LCV RECREATIONAL VAN|            4|421|\n",
      "|2015|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|            4|392|\n",
      "|2016|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|            4|356|\n",
      "|2006|      FIAT|         FIAT DUCATO|    DUCATO 18/MAXI|  DIESEL|LCV RECREATIONAL VAN|            4|318|\n",
      "|2017|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|            4|297|\n",
      "|2012|      FIAT|         FIAT DUCATO|    DUCATO 40/MAXI|  DIESEL|LCV RECREATIONAL VAN|            4|270|\n",
      "|2019|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|            4|268|\n",
      "|2020|      FIAT|         FIAT DUCATO|DUCATO UNSPECIFIED|  DIESEL|LCV RECREATIONAL VAN|            4|268|\n",
      "|2014|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|            4|261|\n",
      "|2013|      FIAT|         FIAT DUCATO|    DUCATO 40/MAXI|  DIESEL|LCV RECREATIONAL VAN|            4|253|\n",
      "|2018|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|            4|248|\n",
      "|2017|      FIAT|         FIAT DUCATO|DUCATO UNSPECIFIED|  DIESEL|LCV RECREATIONAL VAN|            4|247|\n",
      "|2014|      FIAT|         FIAT DUCATO|    DUCATO 40/MAXI|  DIESEL|LCV RECREATIONAL VAN|            4|245|\n",
      "|2007|      FIAT|         FIAT DUCATO|    DUCATO 35/MAXI|  DIESEL|LCV RECREATIONAL VAN|            4|245|\n",
      "|2011|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|            4|239|\n",
      "|2020|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|            4|232|\n",
      "|2018|      FIAT|         FIAT DUCATO|DUCATO UNSPECIFIED|  DIESEL|LCV RECREATIONAL VAN|            4|229|\n",
      "|2019|      FIAT|         FIAT DUCATO|DUCATO UNSPECIFIED|  DIESEL|LCV RECREATIONAL VAN|            4|222|\n",
      "|2012|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|            4|218|\n",
      "|2013|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|            4|215|\n",
      "+----+----------+--------------------+------------------+--------+--------------------+-------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = spark.sql('SELECT year, Make, Model, SubModelShort, FuelType, BodyType, NoofCylinders, count(NoofCylinders) as CC from germany_scrappage group by 1,2,3,4,5,6,7 order by CC DESC')\n",
    "x.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'Make',\n",
       " 'Model',\n",
       " 'SubModelShort',\n",
       " 'FuelType',\n",
       " 'BodyType',\n",
       " 'NoofCylinders',\n",
       " 'CC']"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x.dropDuplicates(subset = ['year',\n",
    " 'Make',\n",
    " 'Model',\n",
    " 'SubModelShort',\n",
    " 'FuelType',\n",
    " 'BodyType',\n",
    " 'NoofCylinders'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80099"
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80099"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.write.csv('x.csv')\n",
    "\n",
    "# x.coalesce(1).write.option(\"header\",True).format(\"csv\").save(\"xx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = ['year','Make', 'Model','SubModelShort', 'FuelType', 'BodyType', 'NoofCylinders']\n",
    "x = df2.groupBy(group_cols).agg(count('NoofCylinders').alias('CC')).orderBy(col('CC').desc(), col('year').asc())\n",
    "# x.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x.dropDuplicates(['year','Make', 'Model','SubModelShort', 'FuelType', 'BodyType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1.filter((col(\"year\")==2002) & (col(\"SubModelShort\")==\"POLO\") & (col(\"BodyType\")==\"HATCHBACK\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+--------------------+--------------+-----------+------------------+-------------+---+----------+\n",
      "|year|         Make|               Model| SubModelShort|   FuelType|          BodyType|NoofCylinders| CC|dense_rank|\n",
      "+----+-------------+--------------------+--------------+-----------+------------------+-------------+---+----------+\n",
      "|1981|       NISSAN|        NISSAN SUNNY|         SUNNY|     PETROL|             SEDAN|            4|  4|         1|\n",
      "|1981|  ROLLS-ROYCE| ROLLS-ROYCE UNSPEC.|   UNSPECIFIED|UNSPECIFIED|       CAR UNSPEC.|         null|  0|         1|\n",
      "|1981|         SAAB|          SAAB 90/99|         90/99|     PETROL|             SEDAN|            4|  5|         1|\n",
      "|1982|MERCEDES-BENZ|MERCEDES-BENZ E-C...|           230|     PETROL|             WAGON|            4|  2|         1|\n",
      "|1982|      PORSCHE|         PORSCHE 911|   911 CARRERA|     PETROL|             COUPE|            6|  1|         1|\n",
      "|1982|         SEAT|        SEAT UNSPEC.|   UNSPECIFIED|UNSPECIFIED|       CAR UNSPEC.|         null|  0|         1|\n",
      "|1982|       TALBOT|TALBOT (MATRA) RA...|(MATRA) RANCHO|     PETROL|ESTATE HIGH VOLUME|            4|  2|         1|\n",
      "|1983|         PUCH|        PUCH G-WAGON|       G-WAGON|     PETROL|          SUV OPEN|            4|  1|         1|\n",
      "|1984|      CITROEN|      CITROEN LN/LNA|        LN/LNA|     PETROL|         HATCHBACK|            2|  1|         1|\n",
      "|1984|       LANCIA|         LANCIA BETA|          BETA|     PETROL|             COUPE|            4|  6|         1|\n",
      "|1984|       TALBOT|       TALBOT TAGORA|        TAGORA|     PETROL|             SEDAN|            4|  1|         1|\n",
      "|1985|          AMC|         AMC UNSPEC.|   UNSPECIFIED|UNSPECIFIED|       CAR UNSPEC.|         null|  0|         1|\n",
      "|1985|   MITSUBISHI|   MITSUBISHI CORDIA|        CORDIA|     PETROL|             COUPE|            4|  4|         1|\n",
      "|1986|      BENTLEY|     BENTLEY UNSPEC.|   UNSPECIFIED|     PETROL|       CAR UNSPEC.|            8|  4|         1|\n",
      "|1986|       BITTER|           BITTER SC|            SC|     PETROL|       CONVERTIBLE|            6|  4|         1|\n",
      "|1986|         FORD|         FORD SIERRA|        SIERRA|     PETROL|             WAGON|            4| 13|         1|\n",
      "|1986|       TOYOTA|      TOYOTA MODEL F|       MODEL F|     PETROL|     LCV COMBI VAN|            4|  2|         1|\n",
      "|1987|          AMC|         AMC UNSPEC.|   UNSPECIFIED|     PETROL|       CAR UNSPEC.|            8|  2|         1|\n",
      "|1987|         AUDI|            AUDI 100|           100|     DIESEL|             SEDAN|            5|  7|         1|\n",
      "|1987|         FORD|        FORD GRANADA|       GRANADA|     PETROL|             WAGON|            4|  1|         1|\n",
      "+----+-------------+--------------------+--------------+-----------+------------------+-------------+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = df2.select(\"year\").distinct().collect()\n",
    "years = sorted([row[x]['year'] for x in range(len(row))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrappage_year_by_age(age):\n",
    "    \"\"\"\n",
    "    This will create a dataframe of last N number of years based on \"age\" parameter\n",
    "    \n",
    "    Args:\n",
    "        df (pandas dataframe): [new reg dataframe]\n",
    "\n",
    "    Returns:\n",
    "        [pandas dataframe]: [Sales percentage contribution of newreg dataframe]\n",
    "    \"\"\"\n",
    "    df = spark.createDataFrame([], schema)\n",
    "\n",
    "    for year in [2000]:\n",
    "        lst = []\n",
    "        for e in range(age * 2 + 1):\n",
    "            lst.append(year - e)\n",
    "\n",
    "        df.withColumn(\"historic_years\",  F.array( [F.lit(x) for x in lst] ))\n",
    "        \n",
    "        # df[\"historic_years\" + \"_\" + year.astype(str)] = (pd.Series(lst).sort_values(ascending=True).reset_index(drop=True))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "  StructField('historic_years', IntegerType(), True)\n",
    "  # StructField('middlename', StringType(), True),\n",
    "  # StructField('lastname', StringType(), True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Make',\n",
       " 'RegistrationType',\n",
       " 'Model',\n",
       " 'SubModelShort',\n",
       " 'BodyType',\n",
       " 'FuelType',\n",
       " 'DrivenWheels',\n",
       " 'Turbo',\n",
       " 'year',\n",
       " 'NoofDoors',\n",
       " 'Engineltr',\n",
       " 'Engineccm',\n",
       " 'EnginekW',\n",
       " 'GrossVehicleWeight',\n",
       " 'NoofCylinders',\n",
       " 'sales']"
      ]
     },
     "execution_count": 651,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|first(year)|\n",
      "+-----------+\n",
      "|       2021|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(first(\"year\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o9237.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 612.0 failed 1 times, most recent failure: Lost task 0.0 in stage 612.0 (TID 17911) (INGGNLT1PYD593.INFO.CORP executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 29 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-648-6d673bdd1569>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscrappage_year_by_age\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m17\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\Scrappage\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m         \"\"\"\n\u001b[0;32m    676\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 677\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    678\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Scrappage\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Scrappage\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Scrappage\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o9237.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 612.0 failed 1 times, most recent failure: Lost task 0.0 in stage 612.0 (TID 17911) (INGGNLT1PYD593.INFO.CORP executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 29 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3519)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3516)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.base/java.net.PlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.base/java.net.PlainSocketImpl.socketAccept(PlainSocketImpl.java:163)\r\n\tat java.base/java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:458)\r\n\tat java.base/java.net.ServerSocket.implAccept(ServerSocket.java:565)\r\n\tat java.base/java.net.ServerSocket.accept(ServerSocket.java:533)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 29 more\r\n"
     ]
    }
   ],
   "source": [
    "scrappage_year_by_age(17).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(np.arange(1981,1990))\n",
    "\n",
    "# [x.cast(IntegerType()) for x in l]\n",
    "# [F.lit(x) for x in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "createDataFrame() missing 1 required positional argument: 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-624-f98fb3266eea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: createDataFrame() missing 1 required positional argument: 'data'"
     ]
    }
   ],
   "source": [
    "F.array([F.lit()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1.to_csv('x1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o8797.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:547)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:587)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:559)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:586)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:559)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:586)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:559)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:705)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:354)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:979)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1814)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1791)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:302)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:326)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)\r\n\t... 21 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-586-809734d00d18>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'True'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\Scrappage\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1370\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[1;32m-> 1372\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1373\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0morc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitionBy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Scrappage\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Scrappage\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Scrappage\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o8797.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:736)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:271)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:287)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:865)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:547)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:587)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:559)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:586)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:559)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:586)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:559)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:705)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:354)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:178)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:979)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1814)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1791)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:302)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:326)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:343)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1039)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1048)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)\r\n\t... 21 more\r\n"
     ]
    }
   ],
   "source": [
    "x.write.options(header='True').csv(\"x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-------+-------------+-----------+-----------+-------------+---+\n",
      "|year|Make|Model  |SubModelShort|FuelType   |BodyType   |NoofCylinders|CC |\n",
      "+----+----+-------+-------------+-----------+-----------+-------------+---+\n",
      "|2005|AUDI|AUDI A4|A4           |UNSPECIFIED|WAGON      |null         |0  |\n",
      "|2005|AUDI|AUDI A4|A4           |UNSPECIFIED|CAR UNSPEC.|null         |0  |\n",
      "|2005|AUDI|AUDI A4|A4           |UNSPECIFIED|SEDAN      |null         |0  |\n",
      "+----+----+-------+-------------+-----------+-----------+-------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.filter((col(\"year\")==2005) & (col(\"SubModelShort\")==\"A4\") & (col(\"FuelType\")==\"UNSPECIFIED\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------------------+------------------+--------+--------------------+---+\n",
      "|year|      Make|               Model|     SubModelShort|FuelType|            BodyType| CC|\n",
      "+----+----------+--------------------+------------------+--------+--------------------+---+\n",
      "|2005|      FIAT|         FIAT DUCATO|    DUCATO 18/MAXI|  DIESEL|LCV RECREATIONAL VAN|421|\n",
      "|2015|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|392|\n",
      "|2016|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|356|\n",
      "|2006|      FIAT|         FIAT DUCATO|    DUCATO 18/MAXI|  DIESEL|LCV RECREATIONAL VAN|318|\n",
      "|2017|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|298|\n",
      "|2012|      FIAT|         FIAT DUCATO|    DUCATO 40/MAXI|  DIESEL|LCV RECREATIONAL VAN|270|\n",
      "|2019|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|268|\n",
      "|2020|      FIAT|         FIAT DUCATO|DUCATO UNSPECIFIED|  DIESEL|LCV RECREATIONAL VAN|268|\n",
      "|2010|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|264|\n",
      "|2014|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|261|\n",
      "|2001|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|       LCV PANEL VAN|259|\n",
      "|2013|      FIAT|         FIAT DUCATO|    DUCATO 40/MAXI|  DIESEL|LCV RECREATIONAL VAN|253|\n",
      "|2018|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|248|\n",
      "|2017|      FIAT|         FIAT DUCATO|DUCATO UNSPECIFIED|  DIESEL|LCV RECREATIONAL VAN|247|\n",
      "|2014|      FIAT|         FIAT DUCATO|    DUCATO 40/MAXI|  DIESEL|LCV RECREATIONAL VAN|245|\n",
      "|2007|      FIAT|         FIAT DUCATO|    DUCATO 35/MAXI|  DIESEL|LCV RECREATIONAL VAN|245|\n",
      "|2011|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|242|\n",
      "|2009|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|241|\n",
      "|2001|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|       LCV COMBI VAN|233|\n",
      "|2020|VOLKSWAGEN|VOLKSWAGEN TRANSP...|       TRANSPORTER|  DIESEL|LCV VAN/TRUCK UNS...|232|\n",
      "+----+----------+--------------------+------------------+--------+--------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy(['year','Make', 'Model','SubModelShort', 'FuelType', 'BodyType']).agg({'NoofCylinders':'count'}).withColumnRenamed('count(NoofCylinders)','CC').orderBy('CC', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------------------+--------------+---------------+--------------------+------------------+\n",
      "|year|      Make|               Model| SubModelShort|       FuelType|            BodyType|sum(NoofCylinders)|\n",
      "+----+----------+--------------------+--------------+---------------+--------------------+------------------+\n",
      "|2006|ALFA ROMEO|      ALFA ROMEO 166|           166|         DIESEL|               SEDAN|                25|\n",
      "|1985|       AMC|         AMC UNSPEC.|   UNSPECIFIED|    UNSPECIFIED|         CAR UNSPEC.|              null|\n",
      "|2004|      AUDI|             AUDI A4|            A4|         DIESEL|               WAGON|               276|\n",
      "|2021|      AUDI|             AUDI A3|            S3|         PETROL|               SEDAN|                 8|\n",
      "|2017|      AUDI|             AUDI A5|            A5|         DIESEL|           HATCHBACK|               242|\n",
      "|2021|      AUDI|        AUDI UNSPEC.|   UNSPECIFIED|ELECTRIC W/OREX|         CAR UNSPEC.|                 0|\n",
      "|1998|       BMW|        BMW 3-SERIES|   318 COMPACT|         DIESEL|           HATCHBACK|                 8|\n",
      "|2012|       BMW|        BMW 3-SERIES|           316|         PETROL|               COUPE|                12|\n",
      "|1987|       BMW|        BMW 3-SERIES|           324|         DIESEL|               SEDAN|                18|\n",
      "|2020|       BMW|        BMW 3-SERIES|      3-SERIES|         DIESEL|               SEDAN|                18|\n",
      "|1995|       BMW|              BMW Z3|            Z3|         PETROL|            ROADSTER|                24|\n",
      "|2013|       BMW|        BMW 5-SERIES|           535|         DIESEL|               SEDAN|                36|\n",
      "|2016|       BMW|        BMW 5-SERIES|          M550|         DIESEL|               SEDAN|                12|\n",
      "|2015|   CITROEN|CITROEN JUMPER/RELAY|     JUMPER 33|         DIESEL|LCV VAN/TRUCK UNS...|                76|\n",
      "|2018|   CITROEN|  CITROEN C4 PICASSO|C4 SPACETOURER|         DIESEL|           MONOSPACE|                80|\n",
      "|1985|   CITROEN|          CITROEN CX|            CX|         PETROL|               SEDAN|                20|\n",
      "|2004|    DAEWOO|       DAEWOO NUBIRA|        NUBIRA|         PETROL|               SEDAN|                36|\n",
      "|1993|       DAF|DAF/LEYLAND 200-4...|    400-SERIES|         DIESEL|       LCV COMBI VAN|                24|\n",
      "|1992|       DAF|DAF/LEYLAND 200-4...|    400-SERIES|         DIESEL|         LCV VAN BUS|                24|\n",
      "|2017|      FIAT|          FIAT DOBLO|         DOBLO|         DIESEL|          CAR PICKUP|                32|\n",
      "+----+----------+--------------------+--------------+---------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m1.write.format(\"csv\").option(\"path\", \"C:\\\\Users\\\\akash.mathur\\\\Downloads\\\\Akash\\\\Spark\\\\m1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1.write.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x.write.option(\"header\",True).csv(\"data/csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = spark.createDataFrame(data).toDF(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame()()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFromData2 = spark.createDataFrame(data).toDF(*columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Make' in df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "54a5a99c93d83e557357eb37914437c813c6d043c207cb2c8427c87ea521d8dd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('Scrappage')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
